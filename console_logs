[32m2024-12-22 02:29:59[0m [34m[1mDEBUG   [0m[34mdist_logger.py:patch_logger:48          [0m [34m[1m[PATCH] logger patched, use (error|warning|info|debug)_(rank_0|all_ranks) instead of the original to avoid unexpected behavior[0m
[32m2024-12-22 02:30:01[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:47   [0m [34m[1m(rank6/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-22 02:30:01[0m [33m[1mWARNING [0m[34mconfig.py:check_args_compatibility:37   [0m [33m[1mparallel_output is disabeld only when sequence_parallel is disabled, now setting sequence_parallel to false[0m
[32m2024-12-22 02:30:01[0m [34m[1mDEBUG   [0m[34mconfig.py:check_args_compatibility:58   [0m [34m[1mpadded_vocab_size is set to 32000[0m
[32m2024-12-22 02:30:01[0m [34m[1mDEBUG   [0m[34mwrap.py:handle_display:55               [0m [34m[1m[PATCH] python builtin print patched, all print() will go to debug_all_ranks[0m
[32m2024-12-22 02:30:01[0m [34m[1mDEBUG   [0m[34mwrap.py:handle_display:61               [0m [34m[1m[PATCH] all logging handlers are removed, logging funcs no longer logs anything[0m
[32m2024-12-22 02:30:01[0m [34m[1mDEBUG   [0m[34mwrap.py:handle_display:66               [0m [34m[1m[PATCH] FutureWarning, UserWarning are ignored[0m
[32m2024-12-22 02:30:01[0m [1mINFO    [0m[34mwrap.py:initialize:31                   [0m [1m[STATUS] initialization started[0m
[32m2024-12-22 02:30:01[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:43   [0m [34m[1m[WRAP] added '/gpfs/public/infra/qhu/projects/megatron_core_080' to sys.path[0m
[32m2024-12-22 02:30:01[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:47   [0m [34m[1m(rank0/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-22 02:30:01[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:47   [0m [34m[1m(rank4/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-22 02:30:01[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:47   [0m [34m[1m(rank3/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-22 02:30:01[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:47   [0m [34m[1m(rank7/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-22 02:30:01[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:47   [0m [34m[1m(rank5/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-22 02:30:01[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:47   [0m [34m[1m(rank2/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-22 02:30:01[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:47   [0m [34m[1m(rank1/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-22 02:30:07[0m [34m[1mDEBUG   [0m[34marguments.py:validate_args:184          [0m [34m[1m(rank0/8) using world size: 8, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 4, pipeline-model-parallel size: 1 [0m
[32m2024-12-22 02:30:07[0m [34m[1mDEBUG   [0m[34marguments.py:validate_args:276          [0m [34m[1m(rank0/8) WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication[0m
[32m2024-12-22 02:30:07[0m [34m[1mDEBUG   [0m[34marguments.py:validate_args:307          [0m [34m[1m(rank0/8) accumulate and all-reduce gradients in fp32 for bfloat16 data type.[0m
[32m2024-12-22 02:30:07[0m [34m[1mDEBUG   [0m[34marguments.py:validate_args:311          [0m [34m[1m(rank0/8) using torch.bfloat16 for parameters ...[0m
[32m2024-12-22 02:30:07[0m [1mINFO    [0m[34mwrap.py:<lambda>:74                     [0m [1m
---------------------------- megatron-lm arguments ----------------------------
accumulate_allreduce_grads_in_fp32 ......................................... False
adam_beta1 ................................................................... 0.9
adam_beta2 .................................................................. 0.95
adam_eps ................................................................... 1e-08
add_bias_linear ............................................................ False
add_position_embedding ..................................................... False
add_qkv_bias ............................................................... False
adlr_autoresume ............................................................ False
adlr_autoresume_interval .................................................... 1000
app_tag_run_name ............................................................ None
app_tag_run_version ........................................................ 0.0.0
apply_layernorm_1p ......................................................... False
apply_query_key_layer_scaling .............................................. False
apply_residual_connection_post_layernorm ................................... False
apply_rope_fusion ........................................................... True
async_save .................................................................. None
async_tensor_model_parallel_allreduce ....................................... True
attention_dropout ............................................................ 0.0
attention_softmax_in_fp32 ................................................... True
auto_detect_ckpt_format .................................................... False
barrier_with_L1_time ........................................................ True
batch_size .................................................................. None
bert_binary_head ............................................................ True
bert_embedder_type ...................................................... megatron
bert_load ................................................................... None
bf16 ........................................................................ True
bias_activation_fusion ..................................................... False
bias_dropout_fusion ......................................................... True
bias_gelu_fusion ............................................................ True
bias_swiglu_fusion .......................................................... True
biencoder_projection_dim ....................................................... 0
biencoder_shared_query_context_model ....................................... False
block_data_path ............................................................. None
calculate_per_token_loss ................................................... False
check_for_nan_in_loss_and_grad .............................................. True
check_weight_hash_across_dp_replicas_interval ............................... None
checkpoint_activations ...................................................... None
ckpt_assume_constant_structure ............................................. False
ckpt_fully_parallel_load ................................................... False
ckpt_fully_parallel_save .................................................... True
ckpt_fully_parallel_save_deprecated ........................................ False
ckpt_step ................................................................... None
classes_fraction ............................................................. 1.0
clip_grad .................................................................... 1.0
clone_scatter_output_in_embedding ........................................... True
consumed_train_samples ......................................................... 0
consumed_valid_samples ......................................................... 0
context_parallel_size .......................................................... 1
create_attention_mask_in_dataloader ......................................... True
cross_entropy_loss_fusion .................................................. False
data_cache_path ............................................................. None
data_parallel_random_init .................................................. False
data_parallel_size .......................................................... None
data_path ................................................................... None
data_per_class_fraction ...................................................... 1.0
data_sharding ............................................................... True
dataloader_type ........................................................... cyclic
ddp_average_in_collective .................................................. False
ddp_bucket_size ............................................................. None
decoder_num_layers .......................................................... None
decoder_seq_length .......................................................... None
decoupled_lr ................................................................ None
decoupled_min_lr ............................................................ None
defer_embedding_wgrad_compute .............................................. False
delay_grad_reduce ........................................................... True
delay_param_gather ......................................................... False
deterministic_mode ......................................................... False
dino_bottleneck_size ......................................................... 256
dino_freeze_last_layer ......................................................... 1
dino_head_hidden_size ....................................................... 2048
dino_local_crops_number ....................................................... 10
dino_local_img_size ........................................................... 96
dino_norm_last_layer ....................................................... False
dino_teacher_temp ........................................................... 0.07
dino_warmup_teacher_temp .................................................... 0.04
dino_warmup_teacher_temp_epochs ............................................... 30
disable_straggler_on_startup ............................................... False
dist_ckpt_format ...................................................... torch_dist
dist_ckpt_strictness ........................................ assume_ok_unexpected
distribute_saved_activations ............................................... False
distributed_backend ......................................................... nccl
distributed_timeout_minutes ................................................... 60
embedding_path .............................................................. None
empty_unused_memory_level ...................................................... 0
enable_one_logger ........................................................... True
encoder_num_layers .......................................................... None
encoder_seq_length .......................................................... None
end_weight_decay ............................................................ None
eod_mask_loss .............................................................. False
eval_interval ............................................................... None
eval_iters .................................................................. None
evidence_data_path .......................................................... None
exit_duration_in_mins ....................................................... None
exit_interval ............................................................... None
exit_on_missing_checkpoint .................................................. True
exit_signal_handler ........................................................ False
expert_model_parallel_size ..................................................... 1
ffn_hidden_size ............................................................ 11008
finetune .................................................................... True
fp16 ....................................................................... False
fp16_lm_cross_entropy ...................................................... False
fp32_residual_connection ................................................... False
fp8 ......................................................................... None
fp8_amax_compute_algo ........................................................ max
fp8_amax_history_len ........................................................ 1024
fp8_interval ................................................................... 1
fp8_margin ..................................................................... 0
fp8_wgrad ................................................................... True
gated_linear_unit .......................................................... False
global_batch_size ............................................................ 128
gradient_accumulation_fusion ................................................ True
group_query_attention ...................................................... False
head_lr_mult ................................................................. 1.0
hidden_dropout ............................................................... 0.0
hidden_size ................................................................. 4096
hybrid_attention_ratio ....................................................... 0.0
hybrid_mlp_ratio ............................................................. 0.0
hybrid_override_pattern ..................................................... None
hysteresis ..................................................................... 2
ict_head_size ............................................................... None
ict_load .................................................................... None
img_h ........................................................................ 224
img_w ........................................................................ 224
indexer_batch_size ........................................................... 128
indexer_log_interval ........................................................ 1000
inference_batch_times_seqlen_threshold ....................................... 512
init_method_std ............................................................ 0.006
init_method_xavier_uniform ................................................. False
initial_loss_scale .................................................... 4294967296
iter_per_epoch .............................................................. 1250
kv_channels ................................................................. None
lazy_mpu_init ............................................................... None
load ................................................ ckpt/llama-2-7b-mcore-tp4pp1
local_rank ..................................................................... 0
log_batch_size_to_tensorboard ............................................... True
log_interval ................................................................... 1
log_learning_rate_to_tensorboard ............................................ True
log_loss_scale_to_tensorboard ............................................... True
log_memory_to_tensorboard .................................................. False
log_num_zeros_in_grad ....................................................... True
log_params_norm ............................................................. True
log_progress ............................................................... False
log_straggler .............................................................. False
log_throughput .............................................................. True
log_timers_to_tensorboard ................................................... True
log_validation_ppl_to_tensorboard .......................................... False
log_world_size_to_tensorboard .............................................. False
logging_level ............................................................... None
loss_scale .................................................................. None
loss_scale_window ........................................................... 1000
lr ......................................................................... 2e-05
lr_decay_iters .............................................................. None
lr_decay_samples ............................................................ None
lr_decay_style ............................................................ cosine
lr_warmup_fraction .......................................................... 0.05
lr_warmup_init ............................................................... 0.0
lr_warmup_iters ................................................................ 0
lr_warmup_samples .............................................................. 0
lr_wsd_decay_iters .......................................................... None
lr_wsd_decay_samples ........................................................ None
lr_wsd_decay_style ................................................... exponential
make_vocab_size_divisible_by ................................................. 100
manual_gc .................................................................. False
manual_gc_eval .............................................................. True
manual_gc_interval ............................................................. 0
mask_factor .................................................................. 1.0
mask_prob ................................................................... 0.15
mask_type ................................................................. random
masked_softmax_fusion ...................................................... False
max_position_embeddings ..................................................... 4096
max_tokens_to_oom .......................................................... 12000
merge_file .................................................................. None
micro_batch_size ............................................................... 4
min_loss_scale ............................................................... 1.0
min_lr ....................................................................... 0.0
mmap_bin_files .............................................................. True
mock_data .................................................................. False
model_parallel_size ......................................................... None
moe_aux_loss_coeff ............................................................. 0
moe_expert_capacity_factor .................................................. None
moe_extended_tp ............................................................ False
moe_grouped_gemm ........................................................... False
moe_input_jitter_eps ........................................................ None
moe_layer_recompute ........................................................ False
moe_pad_expert_input_to_capacity ........................................... False
moe_per_layer_logging ...................................................... False
moe_router_load_balancing_type .......................................... aux_loss
moe_router_pre_softmax ..................................................... False
moe_router_topk ................................................................ 2
moe_token_dispatcher_type .............................................. allgather
moe_token_drop_policy ...................................................... probs
moe_z_loss_coeff ............................................................ None
nccl_communicator_config_path ............................................... None
no_load_optim ............................................................... True
no_load_rng ................................................................. True
no_persist_layer_norm ...................................................... False
no_save_optim ............................................................... True
no_save_rng ................................................................. True
no_sync_func ................................................................ None
norm_epsilon ............................................................... 1e-05
normalization ............................................................ RMSNorm
num_attention_heads ........................................................... 32
num_channels ................................................................... 3
num_classes ................................................................. 1000
num_dataset_builder_threads .................................................... 1
num_experts ................................................................. None
num_layers .................................................................... 32
num_layers_per_virtual_pipeline_stage ....................................... None
num_microbatches_with_partial_activation_checkpoints ........................ None
num_moe_experts ............................................................. None
num_query_groups ............................................................ None
num_workers .................................................................... 8
one_logger_async ........................................................... False
one_logger_project ................................................... megatron-lm
one_logger_run_name ......................................................... None
onnx_safe ................................................................... None
openai_gelu ................................................................ False
optimizer ................................................................... adam
output_bert_embeddings ..................................................... False
overlap_grad_reduce ........................................................ False
overlap_p2p_comm ............................................................ True
overlap_param_gather ....................................................... False
override_opt_param_scheduler ................................................ True
padded_vocab_size .......................................................... 32000
param_sync_func ............................................................. None
params_dtype ................................................................ None
patch_dim ..................................................................... 16
perform_initialization ...................................................... True
pipeline_dtype .............................................................. None
pipeline_model_parallel_size ................................................... 1
pipeline_model_parallel_split_rank .......................................... None
position_embedding_type ......................................... learned_absolute
pretrained_checkpoint ....................................................... None
profile .................................................................... False
profile_ranks ................................................................ [0]
profile_step_end .............................................................. 12
profile_step_start ............................................................ 10
qk_layernorm ............................................................... False
query_in_block_prob .......................................................... 0.1
rampup_batch_size ........................................................... None
rank ........................................................................... 0
recompute_activations ...................................................... False
recompute_granularity ....................................................... None
recompute_method ............................................................ None
recompute_num_layers ........................................................ None
reset_attention_mask ....................................................... False
reset_position_ids ......................................................... False
retriever_report_topk_accuracies .............................................. []
retriever_score_scaling .................................................... False
retriever_seq_length ......................................................... 256
retro_add_retriever ........................................................ False
retro_attention_gate ........................................................... 1
retro_cyclic_train_iters .................................................... None
retro_encoder_attention_dropout .............................................. 0.1
retro_encoder_hidden_dropout ................................................. 0.1
retro_encoder_layers ........................................................... 2
retro_num_neighbors ............................................................ 2
retro_num_retrieved_chunks ..................................................... 2
retro_project_dir ........................................................... None
retro_verify_neighbor_count ................................................. True
rotary_base ................................................................ 10000
rotary_interleaved ......................................................... False
rotary_percent ............................................................... 1.0
rotary_seq_len_interpolation_factor ......................................... None
s3_cache_path ............................................................... None
sample_rate .................................................................. 1.0
save ............................................................ ckpt/test_save_1
save_interval .................................................................. 1
scatter_gather_tensors_in_pipeline .......................................... True
seed ........................................................................ 1234
seq_length ................................................................... 512
sequence_parallel .......................................................... False
sgd_momentum ................................................................. 0.9
short_seq_prob ............................................................... 0.1
skip_train ................................................................. False
spec ........................................................................ None
split .................................................................... 100,0,0
squared_relu ............................................................... False
standalone_embedding_stage ................................................. False
start_weight_decay .......................................................... None
straggler_ctrlr_port ....................................................... 65535
straggler_minmax_count ......................................................... 1
swiglu ...................................................................... True
swin_backbone_type .......................................................... tiny
tensor_model_parallel_size ..................................................... 4
tensorboard_dir ............................................................. None
tensorboard_log_interval ....................................................... 1
tensorboard_queue_size ...................................................... 1000
test_data_path .............................................................. None
test_mode .................................................................. False
tiktoken_num_special_tokens ................................................. 1000
tiktoken_pattern ............................................................ None
tiktoken_special_tokens ..................................................... None
timing_log_level ............................................................... 0
timing_log_option ......................................................... minmax
titles_data_path ............................................................ None
tokenizer_model ............................................................. None
tokenizer_type ......................................... GPTSentencePieceTokenizer
tp_comm_bulk_dgrad .......................................................... True
tp_comm_bulk_wgrad .......................................................... True
tp_comm_overlap ............................................................ False
tp_comm_overlap_ag .......................................................... True
tp_comm_overlap_cfg ......................................................... None
tp_comm_overlap_rs .......................................................... True
tp_comm_overlap_rs_dgrad ................................................... False
tp_comm_split_ag ............................................................ True
tp_comm_split_rs ............................................................ True
train_data_path ............................................................. None
train_iters ................................................................... 64
train_samples ............................................................... None
transformer_impl .............................................. transformer_engine
untie_embeddings_and_output_weights ......................................... True
use_checkpoint_args ........................................................ False
use_checkpoint_opt_param_scheduler ......................................... False
use_cpu_initialization ...................................................... None
use_dist_ckpt .............................................................. False
use_distributed_optimizer ................................................... True
use_flash_attn .............................................................. True
use_legacy_models .......................................................... False
use_one_sent_docs .......................................................... False
use_ring_exchange_p2p ...................................................... False
use_rotary_position_embeddings .............................................. True
use_tp_pp_dp_mapping ....................................................... False
valid_data_path ............................................................. None
variable_seq_lengths ....................................................... False
virtual_pipeline_model_parallel_size ........................................ None
vision_backbone_type ......................................................... vit
vision_pretraining ......................................................... False
vision_pretraining_type ................................................. classify
vocab_extra_ids ................................................................ 0
vocab_file .................................................................. None
vocab_size ................................................................. 32000
wandb_exp_name .............................................................. None
wandb_project ............................................................... None
wandb_save_dir .............................................................. None
warmup ...................................................................... None
weight_decay ................................................................. 0.0
weight_decay_incr_style ................................................. constant
wgrad_deferral_limit ........................................................... 0
world_size ..................................................................... 8
yaml_cfg .................................................................... None
--------------------------------------------------------------------------------
[0m
[32m2024-12-22 02:30:07[0m [34m[1mDEBUG   [0m[34mwrap.py:megatron_lm_initialize:101      [0m [34m[1m[STATUS] initializing torch.distributed[0m
[32m2024-12-22 02:30:07[0m [34m[1mDEBUG   [0m[34minitialize.py:_initialize_distributed:231[0m [34m[1m(rank0/8) > initializing torch distributed ...[0m
[32m2024-12-22 02:30:07[0m [34m[1mDEBUG   [0m[34mglobal_vars.py:_set_one_logger:184      [0m [34m[1m(rank7/8) WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it[0m
[32m2024-12-22 02:30:08[0m [34m[1mDEBUG   [0m[34minitialize.py:_initialize_distributed:268[0m [34m[1m(rank0/8) > initialized tensor model parallel with size 4[0m
[32m2024-12-22 02:30:08[0m [34m[1mDEBUG   [0m[34minitialize.py:_initialize_distributed:272[0m [34m[1m(rank0/8) > initialized pipeline model parallel with size 1[0m
[32m2024-12-22 02:30:08[0m [34m[1mDEBUG   [0m[34mwrap.py:megatron_lm_initialize:104      [0m [34m[1msetting random seeds: 1234[0m
[rank4]:[W1222 02:30:08.492417553 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[32m2024-12-22 02:30:08[0m [34m[1mDEBUG   [0m[34mwrap.py:megatron_lm_initialize:107      [0m [34m[1m[STATUS] initializing auto resume[0m
[32m2024-12-22 02:30:08[0m [34m[1mDEBUG   [0m[34mwrap.py:megatron_lm_initialize:119      [0m [34m[1m[STATUS] setting jit fusion options[0m
[rank0]:[W1222 02:30:08.498581496 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank1]:[W1222 02:30:08.644317994 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank5]:[W1222 02:30:08.645260129 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank2]:[W1222 02:30:09.678184254 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank6]:[W1222 02:30:09.680879373 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank3]:[W1222 02:30:09.699637376 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank7]:[W1222 02:30:09.700920178 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[32m2024-12-22 02:30:11[0m [1mINFO    [0m[34mwrap.py:initialize:37                   [0m [1m(rank1/8) [STATUS] initialization finished, parallel state of this rank: (TP1/4 PP0/1 DP0/2 CP0/1 EP0/1)[0m
[32m2024-12-22 02:30:11[0m [1mINFO    [0m[34mwrap.py:initialize:37                   [0m [1m(rank5/8) [STATUS] initialization finished, parallel state of this rank: (TP1/4 PP0/1 DP1/2 CP0/1 EP0/1)[0m
[32m2024-12-22 02:30:11[0m [34m[1mDEBUG   [0m[34mwrap.py:patch_get_parallel_state:158    [0m [34m[1m[PATCH] the series of get parallel state funcs are patched, use (t|p|d|c|e)p_(rank|size) instead of the original to save effort[0m
[32m2024-12-22 02:30:11[0m [1mINFO    [0m[34mwrap.py:initialize:37                   [0m [1m(rank0/8) [STATUS] initialization finished, parallel state of this rank: (TP0/4 PP0/1 DP0/2 CP0/1 EP0/1)[0m
[32m2024-12-22 02:30:11[0m [1mINFO    [0m[34mmodel.py:_model_:29                     [0m [1m[STATUS] building model[0m
[32m2024-12-22 02:30:11[0m [34m[1mDEBUG   [0m[34mmodel.py:_model_:30                     [0m [34m[1muse transformer engine: True, model provider args: ConfigNestNamespace(model_type='GPT', parallel_output=False, show_weight_details=True, encoder_decoder_type='encoder_or_decoder')[0m
[32m2024-12-22 02:30:11[0m [34m[1mDEBUG   [0m[34mtraining.py:get_model:429               [0m [34m[1m(rank1/8)  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1684803584[0m
[32m2024-12-22 02:30:11[0m [1mINFO    [0m[34mwrap.py:initialize:37                   [0m [1m(rank3/8) [STATUS] initialization finished, parallel state of this rank: (TP3/4 PP0/1 DP0/2 CP0/1 EP0/1)[0m
[32m2024-12-22 02:30:11[0m [1mINFO    [0m[34mwrap.py:initialize:37                   [0m [1m(rank4/8) [STATUS] initialization finished, parallel state of this rank: (TP0/4 PP0/1 DP1/2 CP0/1 EP0/1)[0m
[32m2024-12-22 02:30:11[0m [1mINFO    [0m[34mwrap.py:initialize:37                   [0m [1m(rank2/8) [STATUS] initialization finished, parallel state of this rank: (TP2/4 PP0/1 DP0/2 CP0/1 EP0/1)[0m
[32m2024-12-22 02:30:11[0m [1mINFO    [0m[34mwrap.py:initialize:37                   [0m [1m(rank7/8) [STATUS] initialization finished, parallel state of this rank: (TP3/4 PP0/1 DP1/2 CP0/1 EP0/1)[0m
[32m2024-12-22 02:30:11[0m [34m[1mDEBUG   [0m[34mtraining.py:get_model:429               [0m [34m[1m(rank0/8)  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1684803584[0m
[32m2024-12-22 02:30:11[0m [1mINFO    [0m[34mwrap.py:initialize:37                   [0m [1m(rank6/8) [STATUS] initialization finished, parallel state of this rank: (TP2/4 PP0/1 DP1/2 CP0/1 EP0/1)[0m
[32m2024-12-22 02:30:11[0m [34m[1mDEBUG   [0m[34mutils.py:print_rank_0:269               [0m [34m[1m(rank0/8) > learning rate decay style: cosine[0m
[32m2024-12-22 02:30:11[0m [34m[1mDEBUG   [0m[34mtraining.py:get_model:429               [0m [34m[1m(rank3/8)  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1684803584[0m
[32m2024-12-22 02:30:11[0m [34m[1mDEBUG   [0m[34mtraining.py:get_model:429               [0m [34m[1m(rank2/8)  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1684803584[0m
[32m2024-12-22 02:30:12[0m [34m[1mDEBUG   [0m[34mutils.py:print_rank_0:269               [0m [34m[1m(rank0/8)  loading checkpoint from ckpt/llama-2-7b-mcore-tp4pp1 at iteration 1[0m
[32m2024-12-22 02:30:25[0m [34m[1mDEBUG   [0m[34mutils.py:print_rank_0:269               [0m [34m[1m(rank0/8) could not find arguments in the checkpoint ...[0m
[32m2024-12-22 02:30:28[0m [34m[1mDEBUG   [0m[34mutils.py:print_rank_0:269               [0m [34m[1m(rank0/8)  checkpoint version 3.0[0m
[32m2024-12-22 02:30:28[0m [34m[1mDEBUG   [0m[34mutils.py:print_rank_0:269               [0m [34m[1m(rank0/8)   successfully loaded checkpoint from ckpt/llama-2-7b-mcore-tp4pp1 [ t 0, p 0 ] at iteration 0[0m
[32m2024-12-22 02:30:29[0m [1mINFO    [0m[34mwrap.py:setup_model_and_optimizer:211   [0m [1m[STATUS] model is sucessfully built[0m
[32m2024-12-22 02:30:29[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:220   [0m [34m[1m(rank4/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 92.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 11.8750
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 43.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.0000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.3125
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 62.2500
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.0000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.5000
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.8750
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.8750
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.6250
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.0000
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.0000
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.2500
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.1250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.7500
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.5000
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.0000
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.7500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.6250
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.5000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.2500
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.7500
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.0000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.2500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.0000
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 72.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.7500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.5000
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 94.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.7500
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.0000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.5000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 98.0000
[0m
[32m2024-12-22 02:30:29[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:220   [0m [34m[1m(rank1/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 98.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 13.2500
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 46.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.0000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.7500
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 68.0000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 83.5000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 92.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.6250
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.1250
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.3750
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 84.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.2500
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 25.8750
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.8750
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.7500
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.1250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.5000
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.8750
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.0000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.3750
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.2500
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 71.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.7500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 71.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.2500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 41.7500
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.5000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.7500
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 97.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.0000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
[0m
[32m2024-12-22 02:30:29[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:220   [0m [34m[1m(rank6/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 99.5000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.4375
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 46.7500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.5000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 18.8750
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 90.0000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.5000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.0000
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 89.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.5000
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 83.0000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 83.0000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.1250
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.3750
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.0000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.1250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.0000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.0000
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.1250
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.6250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.5000
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.7500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.2500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.5000
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.2500
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.7500
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.0000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.0000
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.0000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 72.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.2500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 44.0000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 95.5000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 98.5000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
[0m
[32m2024-12-22 02:30:29[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:220   [0m [34m[1m(rank2/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 99.5000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.4375
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 46.7500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.5000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 18.8750
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 90.0000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.5000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.0000
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 89.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.5000
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 83.0000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 83.0000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.1250
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.3750
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.0000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.1250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.0000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.0000
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.1250
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.6250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.5000
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.7500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.2500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.5000
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.2500
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.7500
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.0000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.0000
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.0000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 72.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.2500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 44.0000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 95.5000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 98.5000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
[0m
[32m2024-12-22 02:30:29[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank4/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:30:29[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank1/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:30:29[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:220   [0m [34m[1m(rank0/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 92.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 11.8750
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 43.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.0000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.3125
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 62.2500
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.0000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.5000
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.8750
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.8750
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.6250
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.0000
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.0000
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.2500
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.1250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.7500
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.5000
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.0000
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.7500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.6250
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.5000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.2500
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.7500
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.0000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.2500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.0000
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 72.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.7500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.5000
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 94.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.7500
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.0000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.5000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 98.0000
[0m
[32m2024-12-22 02:30:29[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:220   [0m [34m[1m(rank5/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 98.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 13.2500
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 46.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.0000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.7500
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 68.0000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 83.5000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 92.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.6250
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.1250
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.3750
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 84.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.2500
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 25.8750
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.8750
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.7500
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.la[32m2024-12-22 02:30:29[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:220   [0m [34m[1m(rank3/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 18.3750
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 52.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.5000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 19.0000
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 92.5000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.0000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.7500
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 24.8750
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.3750
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 86.5000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.3750
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 86.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.8750
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.7500
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.7500
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.0000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.6250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.0000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.8750
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.la[32m2024-12-22 02:30:29[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank6/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:30:29[0m [1mINFO    [0m[34mwrap.py:setup_model_and_optimizer:224   [0m [1m[STATUS] optimizer is sucessfully built: DistributedOptimizer(type=adam, lr=2e-05, min_lr=0.0, weight_decay=0.0adam_beta=(0.9,0.95), adam_eps=1e-08, sgd_momentum=0.9[0m
[32m2024-12-22 02:30:29[0m [1mINFO    [0m[34mwrap.py:setup_model_and_optimizer:225   [0m [1m[STATUS] scheduler is sucessfully built: OptimizerParamScheduler(lr_decay_style=cosine, lr_warmup_steps=409.6, lr_decay_steps=8192)[0m
yers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.1250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.5000
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.8750
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.0000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.3750
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.2500
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 71.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.7500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape =[32m2024-12-22 02:30:29[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank2/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
yers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.6250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.5000
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.0000
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.0000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.0000
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.7500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.7500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 70.0000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.5000
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 71.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.2500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 41.7500
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.5000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.7500
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 97.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.0000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
[0m
 torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.5000
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 70.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.2500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.5000
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.5000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.5000
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 94.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.5000
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.0000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 100.0000
[0m
[32m2024-12-22 02:30:29[0m [1mINFO    [0m[34mwrap.py:_set_megatron_wrap_training_flow:284[0m [1m[STATUS] successfully set megatron wrap training flow: MegatronWrapMinimalMockMSEFlow(flow_key=minimal_mock_mse, seq_length=512, micro_batch_size=4)[0m
[32m2024-12-22 02:30:29[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank0/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:30:29[0m [34m[1mDEBUG   [0m[34mtimers.py:log:369                       [0m [34m[1m(rank7/8) (min, max) time across ranks (ms):
    load-checkpoint ................................: (17128.74, 17128.93)[0m
[32m2024-12-22 02:30:29[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank5/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:30:29[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank3/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:30:29[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:220   [0m [34m[1m(rank7/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 18.3750
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 52.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.5000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 19.0000
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 92.5000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.0000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.7500
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 24.8750
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.3750
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 86.5000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.3750
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 86.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.8750
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.7500
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.7500
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.0000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.6250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.0000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.8750
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.6250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.5000
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.0000
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.0000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.0000
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.7500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.7500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 70.0000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.5000
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.5000
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 70.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.2500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.5000
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.5000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.5000
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 94.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.5000
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.0000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 100.0000
[0m
[32m2024-12-22 02:30:29[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank7/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:30:32[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 0) loss: 80.0595, random_length: 452.0000[0m
[32m2024-12-22 02:30:35[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 1) loss: 109.5258, random_length: 120.0000[0m
[32m2024-12-22 02:30:37[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 2) loss: 114.4262, random_length: 8.0000[0m
[32m2024-12-22 02:30:39[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 3) loss: 107.2352, random_length: 93.0000[0m
[32m2024-12-22 02:30:40[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 4) loss: 136.8486, random_length: 36.0000[0m
[32m2024-12-22 02:30:42[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 5) loss: 103.2843, random_length: 86.0000[0m
[32m2024-12-22 02:30:44[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 6) loss: 108.6465, random_length: 101.0000[0m
[32m2024-12-22 02:30:45[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 7) loss: 79.4162, random_length: 364.0000[0m
[32m2024-12-22 02:30:47[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 8) loss: 86.1738, random_length: 243.0000[0m
[32m2024-12-22 02:30:49[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 9) loss: 114.2661, random_length: 18.0000[0m
[32m2024-12-22 02:30:50[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 10) loss: 130.4630, random_length: 32.0000[0m
[32m2024-12-22 02:30:52[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 11) loss: 104.7106, random_length: 17.0000[0m
[32m2024-12-22 02:30:53[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 12) loss: 81.2028, random_length: 355.0000[0m
[32m2024-12-22 02:30:55[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 13) loss: 79.2434, random_length: 496.0000[0m
[32m2024-12-22 02:30:57[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 14) loss: 75.3494, random_length: 475.0000[0m
[32m2024-12-22 02:30:58[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 15) loss: 92.5865, random_length: 153.0000[0m
[32m2024-12-22 02:31:00[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank3/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:31:00[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank1/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:31:00[0m [1mINFO    [0m[34mwrap.py:log_last_metrics:367            [0m [1miteration 1 | loss 6.263429164886475 | grad_norm 1.2999e+05 | learning_rate 6.2500e-06 | throughput 1.0896e+01 | random_length 1.9056e+02 | mean_elapsed_time 3.0409e+01 | consumed_samples 128 | num_zeros_in_grad 2.6191e+08 | loss_scale 1.0000e+00 | params_norm 1.5886e+03[0m
[32m2024-12-22 02:31:00[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank2/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:31:00[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank0/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:31:00[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank4/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:31:00[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank7/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:31:00[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank5/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:31:00[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank6/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:31:01[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 0) loss: 98.6585, random_length: 94.0000[0m
[32m2024-12-22 02:31:02[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 1) loss: 100.4819, random_length: 188.0000[0m
[32m2024-12-22 02:31:04[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 2) loss: 100.8566, random_length: 118.0000[0m
[32m2024-12-22 02:31:06[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 3) loss: 105.9819, random_length: 15.0000[0m
[32m2024-12-22 02:31:07[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 4) loss: 79.7255, random_length: 499.0000[0m
[32m2024-12-22 02:31:09[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 5) loss: 87.7190, random_length: 256.0000[0m
[32m2024-12-22 02:31:11[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 6) loss: 117.0376, random_length: 67.0000[0m
[32m2024-12-22 02:31:13[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 7) loss: 77.6977, random_length: 478.0000[0m
[32m2024-12-22 02:31:14[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 8) loss: 110.7163, random_length: 71.0000[0m
[32m2024-12-22 02:31:16[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 9) loss: 106.9539, random_length: 92.0000[0m
[32m2024-12-22 02:31:17[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 10) loss: 114.1505, random_length: 48.0000[0m
[32m2024-12-22 02:31:19[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 11) loss: 87.2471, random_length: 277.0000[0m
[32m2024-12-22 02:31:21[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 12) loss: 117.1086, random_length: 65.0000[0m
[32m2024-12-22 02:31:22[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 13) loss: 82.1069, random_length: 310.0000[0m
[32m2024-12-22 02:31:24[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 14) loss: 78.1983, random_length: 493.0000[0m
[32m2024-12-22 02:31:25[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 15) loss: 77.3667, random_length: 364.0000[0m
[32m2024-12-22 02:31:26[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank3/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:31:26[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank5/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:31:26[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank4/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:31:26[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank6/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:31:26[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank2/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:31:26[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank1/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:31:26[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank7/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:31:26[0m [1mINFO    [0m[34mwrap.py:log_last_metrics:367            [0m [1miteration 2 | loss 6.023465156555176 | grad_norm 4.3043e+04 | learning_rate 1.2500e-05 | throughput 1.2892e+01 | random_length 2.1469e+02 | mean_elapsed_time 2.5702e+01 | consumed_samples 256 | num_zeros_in_grad 2.6193e+08 | loss_scale 1.0000e+00 | params_norm 1.5886e+03[0m
[32m2024-12-22 02:31:26[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank0/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:31:27[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 0) loss: 57.2680, random_length: 273.0000[0m
[32m2024-12-22 02:31:28[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 1) loss: 56.1448, random_length: 256.0000[0m
[32m2024-12-22 02:31:28[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 2) loss: 66.6800, random_length: 205.0000[0m
[32m2024-12-22 02:31:30[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 3) loss: 48.8522, random_length: 465.0000[0m
[32m2024-12-22 02:31:32[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 4) loss: 109.5974, random_length: 20.0000[0m
[32m2024-12-22 02:31:34[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 5) loss: 91.9336, random_length: 73.0000[0m
[32m2024-12-22 02:31:35[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 6) loss: 188.6670, random_length: 6.0000[0m
[32m2024-12-22 02:31:37[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 7) loss: 48.4847, random_length: 342.0000[0m
[32m2024-12-22 02:31:39[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 8) loss: 70.5555, random_length: 148.0000[0m
[32m2024-12-22 02:31:40[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 9) loss: 47.7175, random_length: 485.0000[0m
[32m2024-12-22 02:31:42[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 10) loss: 54.7637, random_length: 387.0000[0m
[32m2024-12-22 02:31:44[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 11) loss: 67.0478, random_length: 172.0000[0m
[32m2024-12-22 02:31:45[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 12) loss: 88.1274, random_length: 56.0000[0m
[32m2024-12-22 02:31:47[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 13) loss: 81.9593, random_length: 79.0000[0m
[32m2024-12-22 02:31:49[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 14) loss: 42.6726, random_length: 509.0000[0m
[32m2024-12-22 02:31:50[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 15) loss: 67.1246, random_length: 168.0000[0m
[32m2024-12-22 02:31:52[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank3/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:31:52[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank1/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:31:52[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank2/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:31:52[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank6/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:31:52[0m [1mINFO    [0m[34mwrap.py:log_last_metrics:367            [0m [1miteration 3 | loss 4.639047622680664 | grad_norm 1.3862e+04 | learning_rate 1.8750e-05 | throughput 1.2796e+01 | random_length 2.2775e+02 | mean_elapsed_time 2.5896e+01 | consumed_samples 384 | num_zeros_in_grad 2.6189e+08 | loss_scale 1.0000e+00 | params_norm 1.5886e+03[0m
[32m2024-12-22 02:31:52[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank5/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:31:52[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank4/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:31:52[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank0/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:31:52[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank7/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:31:52[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 0) loss: 16.9998, random_length: 78.0000[0m
[32m2024-12-22 02:31:54[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 1) loss: 7.2283, random_length: 221.0000[0m
[32m2024-12-22 02:31:55[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 2) loss: 8.6391, random_length: 148.0000[0m
[32m2024-12-22 02:31:56[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 3) loss: 30.6628, random_length: 28.0000[0m
[32m2024-12-22 02:31:57[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 4) loss: 5.8446, random_length: 355.0000[0m
[32m2024-12-22 02:31:58[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 5) loss: 12.1631, random_length: 114.0000[0m
[32m2024-12-22 02:32:00[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 6) loss: 10.4520, random_length: 145.0000[0m
[32m2024-12-22 02:32:01[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 7) loss: 5.3224, random_length: 313.0000[0m
[32m2024-12-22 02:32:03[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 8) loss: 4.6022, random_length: 466.0000[0m
[32m2024-12-22 02:32:05[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 9) loss: 24.6095, random_length: 41.0000[0m
[32m2024-12-22 02:32:06[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 10) loss: 4.6066, random_length: 427.0000[0m
[32m2024-12-22 02:32:08[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 11) loss: 5.7753, random_length: 284.0000[0m
[32m2024-12-22 02:32:10[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 12) loss: 17.6115, random_length: 42.0000[0m
[32m2024-12-22 02:32:11[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 13) loss: 7.5346, random_length: 182.0000[0m
[32m2024-12-22 02:32:13[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 14) loss: 5.3661, random_length: 279.0000[0m
[32m2024-12-22 02:32:15[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 15) loss: 8.2942, random_length: 159.0000[0m
[32m2024-12-22 02:32:16[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank4/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:32:16[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank5/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:32:16[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank1/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:32:16[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank6/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:32:16[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank2/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:32:16[0m [1mINFO    [0m[34mwrap.py:log_last_metrics:367            [0m [1miteration 4 | loss 0.6863753199577332 | grad_norm 1.6381e+03 | learning_rate 1.9991e-05 | throughput 1.3731e+01 | random_length 2.0512e+02 | mean_elapsed_time 2.4132e+01 | consumed_samples 512 | num_zeros_in_grad 2.6185e+08 | loss_scale 1.0000e+00 | params_norm 1.5886e+03[0m
[32m2024-12-22 02:32:16[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank0/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:32:16[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank7/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:32:16[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank3/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:32:17[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 0) loss: 2.0197, random_length: 308.0000[0m
[32m2024-12-22 02:32:18[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 1) loss: 2.0479, random_length: 159.0000[0m
[32m2024-12-22 02:32:19[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 2) loss: 2.8770, random_length: 476.0000[0m
[32m2024-12-22 02:32:20[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 3) loss: 4.7108, random_length: 43.0000[0m
[32m2024-12-22 02:32:22[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 4) loss: 1.6742, random_length: 396.0000[0m
[32m2024-12-22 02:32:24[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 5) loss: 1.9756, random_length: 358.0000[0m
[32m2024-12-22 02:32:25[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 6) loss: 2.4542, random_length: 508.0000[0m
[32m2024-12-22 02:32:27[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 7) loss: 1.9374, random_length: 350.0000[0m
[32m2024-12-22 02:32:29[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 8) loss: 2.2294, random_length: 480.0000[0m
[32m2024-12-22 02:32:31[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 9) loss: 2.2622, random_length: 488.0000[0m
[32m2024-12-22 02:32:32[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 10) loss: 5.7587, random_length: 47.0000[0m
[32m2024-12-22 02:32:34[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 11) loss: 2.2034, random_length: 483.0000[0m
[32m2024-12-22 02:32:36[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 12) loss: 2.5330, random_length: 175.0000[0m
[32m2024-12-22 02:32:37[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 13) loss: 2.0198, random_length: 451.0000[0m
[32m2024-12-22 02:32:39[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 14) loss: 2.0154, random_length: 449.0000[0m
[32m2024-12-22 02:32:41[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 15) loss: 2.8874, random_length: 303.0000[0m
[32m2024-12-22 02:32:42[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank1/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:32:42[0m [1mINFO    [0m[34mwrap.py:log_last_metrics:367            [0m [1miteration 5 | loss 0.16252359747886658 | grad_norm 3.7926e+02 | learning_rate 1.9957e-05 | throughput 1.2698e+01 | random_length 3.4212e+02 | mean_elapsed_time 2.6095e+01 | consumed_samples 640 | num_zeros_in_grad 2.6183e+08 | loss_scale 1.0000e+00 | params_norm 1.5886e+03[0m
[32m2024-12-22 02:32:42[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank0/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:32:42[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank6/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:32:42[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank4/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:32:42[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank2/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:32:42[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank5/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:32:42[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank7/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:32:42[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank3/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:32:43[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 0) loss: 8.3271, random_length: 282.0000[0m
[32m2024-12-22 02:32:45[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 1) loss: 6.0222, random_length: 127.0000[0m
[32m2024-12-22 02:32:47[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 2) loss: 7.2987, random_length: 191.0000[0m
[32m2024-12-22 02:32:48[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 3) loss: 7.0163, random_length: 427.0000[0m
[32m2024-12-22 02:32:49[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 4) loss: 7.5018, random_length: 447.0000[0m
[32m2024-12-22 02:32:50[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 5) loss: 5.4166, random_length: 123.0000[0m
[32m2024-12-22 02:32:52[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 6) loss: 6.2868, random_length: 106.0000[0m
[32m2024-12-22 02:32:54[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 7) loss: 7.9535, random_length: 497.0000[0m
[32m2024-12-22 02:32:55[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 8) loss: 8.2364, random_length: 386.0000[0m
[32m2024-12-22 02:32:57[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 9) loss: 7.3269, random_length: 262.0000[0m
[32m2024-12-22 02:32:59[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 10) loss: 6.8602, random_length: 197.0000[0m
[32m2024-12-22 02:33:00[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 11) loss: 8.4070, random_length: 354.0000[0m
[32m2024-12-22 02:33:02[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 12) loss: 5.2188, random_length: 111.0000[0m
[32m2024-12-22 02:33:04[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 13) loss: 6.2210, random_length: 161.0000[0m
[32m2024-12-22 02:33:05[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 14) loss: 6.4591, random_length: 276.0000[0m
[32m2024-12-22 02:33:06[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 15) loss: 7.3523, random_length: 159.0000[0m
[32m2024-12-22 02:33:07[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank5/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:33:07[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank2/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:33:07[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank6/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:33:07[0m [1mINFO    [0m[34mwrap.py:log_last_metrics:367            [0m [1miteration 6 | loss 0.43712785840034485 | grad_norm 8.7161e+02 | learning_rate 1.9896e-05 | throughput 1.3431e+01 | random_length 2.5662e+02 | mean_elapsed_time 2.4670e+01 | consumed_samples 768 | num_zeros_in_grad 2.6192e+08 | loss_scale 1.0000e+00 | params_norm 1.5886e+03[0m
[32m2024-12-22 02:33:07[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank4/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:33:07[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank0/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:33:07[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank3/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:33:07[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank7/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:33:07[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank1/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:33:08[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 0) loss: 1.4246, random_length: 332.0000[0m
[32m2024-12-22 02:33:09[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 1) loss: 2.1082, random_length: 458.0000[0m
[32m2024-12-22 02:33:10[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 2) loss: 2.1779, random_length: 73.0000[0m
[32m2024-12-22 02:33:11[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 3) loss: 1.4403, random_length: 81.0000[0m
[32m2024-12-22 02:33:13[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 4) loss: 1.6498, random_length: 405.0000[0m
[32m2024-12-22 02:33:15[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 5) loss: 1.2170, random_length: 220.0000[0m
[32m2024-12-22 02:33:17[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 6) loss: 1.8804, random_length: 49.0000[0m
[32m2024-12-22 02:33:18[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 7) loss: 0.8749, random_length: 294.0000[0m
[32m2024-12-22 02:33:20[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 8) loss: 2.1380, random_length: 39.0000[0m
[32m2024-12-22 02:33:21[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 9) loss: 1.4237, random_length: 225.0000[0m
[32m2024-12-22 02:33:23[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 10) loss: 1.5781, random_length: 90.0000[0m
[32m2024-12-22 02:33:25[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 11) loss: 1.9279, random_length: 325.0000[0m
[32m2024-12-22 02:33:26[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 12) loss: 1.2154, random_length: 313.0000[0m
[32m2024-12-22 02:33:27[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 13) loss: 1.1468, random_length: 472.0000[0m
[32m2024-12-22 02:33:28[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 14) loss: 1.8130, random_length: 68.0000[0m
[32m2024-12-22 02:33:30[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 15) loss: 1.2513, random_length: 264.0000[0m
[32m2024-12-22 02:33:31[0m [1mINFO    [0m[34mwrap.py:log_last_metrics:367            [0m [1miteration 7 | loss 0.0987008661031723 | grad_norm 2.8796e+02 | learning_rate 1.9808e-05 | throughput 1.3432e+01 | random_length 2.3175e+02 | mean_elapsed_time 2.4668e+01 | consumed_samples 896 | num_zeros_in_grad 2.6191e+08 | loss_scale 1.0000e+00 | params_norm 1.5886e+03[0m
[32m2024-12-22 02:33:31[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank2/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:33:31[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank6/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:33:31[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank4/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:33:31[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank0/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:33:31[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank5/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:33:31[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank3/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:33:31[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank7/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:33:31[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank1/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:33:32[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 0) loss: 3.0476, random_length: 75.0000[0m
[32m2024-12-22 02:33:34[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 1) loss: 3.4494, random_length: 437.0000[0m
[32m2024-12-22 02:33:36[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 2) loss: 3.3929, random_length: 207.0000[0m
