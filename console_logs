[32m2024-12-22 02:25:12[0m [34m[1mDEBUG   [0m[34mdist_logger.py:patch_logger:48          [0m [34m[1m[PATCH] logger patched, use (error|warning|info|debug)_(rank_0|all_ranks) instead of the original to avoid unexpected behavior[0m
[32m2024-12-22 02:25:13[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:47   [0m [34m[1m(rank5/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-22 02:25:13[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:47   [0m [34m[1m(rank1/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-22 02:25:13[0m [33m[1mWARNING [0m[34mconfig.py:check_args_compatibility:37   [0m [33m[1mparallel_output is disabeld only when sequence_parallel is disabled, now setting sequence_parallel to false[0m
[32m2024-12-22 02:25:13[0m [34m[1mDEBUG   [0m[34mconfig.py:check_args_compatibility:58   [0m [34m[1mpadded_vocab_size is set to 32000[0m
[32m2024-12-22 02:25:13[0m [34m[1mDEBUG   [0m[34mwrap.py:handle_display:55               [0m [34m[1m[PATCH] python builtin print patched, all print() will go to debug_all_ranks[0m
[32m2024-12-22 02:25:13[0m [34m[1mDEBUG   [0m[34mwrap.py:handle_display:61               [0m [34m[1m[PATCH] all logging handlers are removed, logging funcs no longer logs anything[0m
[32m2024-12-22 02:25:13[0m [34m[1mDEBUG   [0m[34mwrap.py:handle_display:66               [0m [34m[1m[PATCH] FutureWarning, UserWarning are ignored[0m
[32m2024-12-22 02:25:13[0m [1mINFO    [0m[34mwrap.py:initialize:31                   [0m [1m[STATUS] initialization started[0m
[32m2024-12-22 02:25:13[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:43   [0m [34m[1m[WRAP] added '/gpfs/public/infra/qhu/projects/megatron_core_080' to sys.path[0m
[32m2024-12-22 02:25:13[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:47   [0m [34m[1m(rank0/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-22 02:25:13[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:47   [0m [34m[1m(rank6/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-22 02:25:13[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:47   [0m [34m[1m(rank4/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-22 02:25:13[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:47   [0m [34m[1m(rank7/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-22 02:25:13[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:47   [0m [34m[1m(rank3/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-22 02:25:13[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:47   [0m [34m[1m(rank2/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-22 02:25:19[0m [34m[1mDEBUG   [0m[34marguments.py:validate_args:184          [0m [34m[1m(rank0/8) using world size: 8, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 4, pipeline-model-parallel size: 1 [0m
[32m2024-12-22 02:25:19[0m [34m[1mDEBUG   [0m[34marguments.py:validate_args:276          [0m [34m[1m(rank0/8) WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication[0m
[32m2024-12-22 02:25:19[0m [34m[1mDEBUG   [0m[34marguments.py:validate_args:307          [0m [34m[1m(rank0/8) accumulate and all-reduce gradients in fp32 for bfloat16 data type.[0m
[32m2024-12-22 02:25:19[0m [34m[1mDEBUG   [0m[34marguments.py:validate_args:311          [0m [34m[1m(rank0/8) using torch.bfloat16 for parameters ...[0m
[32m2024-12-22 02:25:19[0m [1mINFO    [0m[34mwrap.py:<lambda>:74                     [0m [1m
---------------------------- megatron-lm arguments ----------------------------
accumulate_allreduce_grads_in_fp32 ......................................... False
adam_beta1 ................................................................... 0.9
adam_beta2 .................................................................. 0.95
adam_eps ................................................................... 1e-08
add_bias_linear ............................................................ False
add_position_embedding ..................................................... False
add_qkv_bias ............................................................... False
adlr_autoresume ............................................................ False
adlr_autoresume_interval .................................................... 1000
app_tag_run_name ............................................................ None
app_tag_run_version ........................................................ 0.0.0
apply_layernorm_1p ......................................................... False
apply_query_key_layer_scaling .............................................. False
apply_residual_connection_post_layernorm ................................... False
apply_rope_fusion ........................................................... True
async_save .................................................................. None
async_tensor_model_parallel_allreduce ....................................... True
attention_dropout ............................................................ 0.0
attention_softmax_in_fp32 ................................................... True
auto_detect_ckpt_format .................................................... False
barrier_with_L1_time ........................................................ True
batch_size .................................................................. None
bert_binary_head ............................................................ True
bert_embedder_type ...................................................... megatron
bert_load ................................................................... None
bf16 ........................................................................ True
bias_activation_fusion ..................................................... False
bias_dropout_fusion ......................................................... True
bias_gelu_fusion ............................................................ True
bias_swiglu_fusion .......................................................... True
biencoder_projection_dim ....................................................... 0
biencoder_shared_query_context_model ....................................... False
block_data_path ............................................................. None
calculate_per_token_loss ................................................... False
check_for_nan_in_loss_and_grad .............................................. True
check_weight_hash_across_dp_replicas_interval ............................... None
checkpoint_activations ...................................................... None
ckpt_assume_constant_structure ............................................. False
ckpt_fully_parallel_load ................................................... False
ckpt_fully_parallel_save .................................................... True
ckpt_fully_parallel_save_deprecated ........................................ False
ckpt_step ................................................................... None
classes_fraction ............................................................. 1.0
clip_grad .................................................................... 1.0
clone_scatter_output_in_embedding ........................................... True
consumed_train_samples ......................................................... 0
consumed_valid_samples ......................................................... 0
context_parallel_size .......................................................... 1
create_attention_mask_in_dataloader ......................................... True
cross_entropy_loss_fusion .................................................. False
data_cache_path ............................................................. None
data_parallel_random_init .................................................. False
data_parallel_size .......................................................... None
data_path ................................................................... None
data_per_class_fraction ...................................................... 1.0
data_sharding ............................................................... True
dataloader_type ........................................................... cyclic
ddp_average_in_collective .................................................. False
ddp_bucket_size ............................................................. None
decoder_num_layers .......................................................... None
decoder_seq_length .......................................................... None
decoupled_lr ................................................................ None
decoupled_min_lr ............................................................ None
defer_embedding_wgrad_compute .............................................. False
delay_grad_reduce ........................................................... True
delay_param_gather ......................................................... False
deterministic_mode ......................................................... False
dino_bottleneck_size ......................................................... 256
dino_freeze_last_layer ......................................................... 1
dino_head_hidden_size ....................................................... 2048
dino_local_crops_number ....................................................... 10
dino_local_img_size ........................................................... 96
dino_norm_last_layer ....................................................... False
dino_teacher_temp ........................................................... 0.07
dino_warmup_teacher_temp .................................................... 0.04
dino_warmup_teacher_temp_epochs ............................................... 30
disable_straggler_on_startup ............................................... False
dist_ckpt_format ...................................................... torch_dist
dist_ckpt_strictness ........................................ assume_ok_unexpected
distribute_saved_activations ............................................... False
distributed_backend ......................................................... nccl
distributed_timeout_minutes ................................................... 60
embedding_path .............................................................. None
empty_unused_memory_level ...................................................... 0
enable_one_logger ........................................................... True
encoder_num_layers .......................................................... None
encoder_seq_length .......................................................... None
end_weight_decay ............................................................ None
eod_mask_loss .............................................................. False
eval_interval ............................................................... None
eval_iters .................................................................. None
evidence_data_path .......................................................... None
exit_duration_in_mins ....................................................... None
exit_interval ............................................................... None
exit_on_missing_checkpoint .................................................. True
exit_signal_handler ........................................................ False
expert_model_parallel_size ..................................................... 1
ffn_hidden_size ............................................................ 11008
finetune .................................................................... True
fp16 ....................................................................... False
fp16_lm_cross_entropy ...................................................... False
fp32_residual_connection ................................................... False
fp8 ......................................................................... None
fp8_amax_compute_algo ........................................................ max
fp8_amax_history_len ........................................................ 1024
fp8_interval ................................................................... 1
fp8_margin ..................................................................... 0
fp8_wgrad ................................................................... True
gated_linear_unit .......................................................... False
global_batch_size ............................................................ 128
gradient_accumulation_fusion ................................................ True
group_query_attention ...................................................... False
head_lr_mult ................................................................. 1.0
hidden_dropout ............................................................... 0.0
hidden_size ................................................................. 4096
hybrid_attention_ratio ....................................................... 0.0
hybrid_mlp_ratio ............................................................. 0.0
hybrid_override_pattern ..................................................... None
hysteresis ..................................................................... 2
ict_head_size ............................................................... None
ict_load .................................................................... None
img_h ........................................................................ 224
img_w ........................................................................ 224
indexer_batch_size ........................................................... 128
indexer_log_interval ........................................................ 1000
inference_batch_times_seqlen_threshold ....................................... 512
init_method_std ............................................................ 0.006
init_method_xavier_uniform ................................................. False
initial_loss_scale .................................................... 4294967296
iter_per_epoch .............................................................. 1250
kv_channels ................................................................. None
lazy_mpu_init ............................................................... None
load ................................................ ckpt/llama-2-7b-mcore-tp4pp1
local_rank ..................................................................... 0
log_batch_size_to_tensorboard ............................................... True
log_interval ................................................................... 1
log_learning_rate_to_tensorboard ............................................ True
log_loss_scale_to_tensorboard ............................................... True
log_memory_to_tensorboard .................................................. False
log_num_zeros_in_grad ....................................................... True
log_params_norm ............................................................. True
log_progress ............................................................... False
log_straggler .............................................................. False
log_throughput .............................................................. True
log_timers_to_tensorboard ................................................... True
log_validation_ppl_to_tensorboard .......................................... False
log_world_size_to_tensorboard .............................................. False
logging_level ............................................................... None
loss_scale .................................................................. None
loss_scale_window ........................................................... 1000
lr ......................................................................... 2e-05
lr_decay_iters .............................................................. None
lr_decay_samples ............................................................ None
lr_decay_style ............................................................ cosine
lr_warmup_fraction .......................................................... 0.05
lr_warmup_init ............................................................... 0.0
lr_warmup_iters ................................................................ 0
lr_warmup_samples .............................................................. 0
lr_wsd_decay_iters .......................................................... None
lr_wsd_decay_samples ........................................................ None
lr_wsd_decay_style ................................................... exponential
make_vocab_size_divisible_by ................................................. 100
manual_gc .................................................................. False
manual_gc_eval .............................................................. True
manual_gc_interval ............................................................. 0
mask_factor .................................................................. 1.0
mask_prob ................................................................... 0.15
mask_type ................................................................. random
masked_softmax_fusion ...................................................... False
max_position_embeddings ..................................................... 4096
max_tokens_to_oom .......................................................... 12000
merge_file .................................................................. None
micro_batch_size ............................................................... 4
min_loss_scale ............................................................... 1.0
min_lr ....................................................................... 0.0
mmap_bin_files .............................................................. True
mock_data .................................................................. False
model_parallel_size ......................................................... None
moe_aux_loss_coeff ............................................................. 0
moe_expert_capacity_factor .................................................. None
moe_extended_tp ............................................................ False
moe_grouped_gemm ........................................................... False
moe_input_jitter_eps ........................................................ None
moe_layer_recompute ........................................................ False
moe_pad_expert_input_to_capacity ........................................... False
moe_per_layer_logging ...................................................... False
moe_router_load_balancing_type .......................................... aux_loss
moe_router_pre_softmax ..................................................... False
moe_router_topk ................................................................ 2
moe_token_dispatcher_type .............................................. allgather
moe_token_drop_policy ...................................................... probs
moe_z_loss_coeff ............................................................ None
nccl_communicator_config_path ............................................... None
no_load_optim ............................................................... True
no_load_rng ................................................................. True
no_persist_layer_norm ...................................................... False
no_save_optim ............................................................... True
no_save_rng ................................................................. True
no_sync_func ................................................................ None
norm_epsilon ............................................................... 1e-05
normalization ............................................................ RMSNorm
num_attention_heads ........................................................... 32
num_channels ................................................................... 3
num_classes ................................................................. 1000
num_dataset_builder_threads .................................................... 1
num_experts ................................................................. None
num_layers .................................................................... 32
num_layers_per_virtual_pipeline_stage ....................................... None
num_microbatches_with_partial_activation_checkpoints ........................ None
num_moe_experts ............................................................. None
num_query_groups ............................................................ None
num_workers .................................................................... 8
one_logger_async ........................................................... False
one_logger_project ................................................... megatron-lm
one_logger_run_name ......................................................... None
onnx_safe ................................................................... None
openai_gelu ................................................................ False
optimizer ................................................................... adam
output_bert_embeddings ..................................................... False
overlap_grad_reduce ........................................................ False
overlap_p2p_comm ............................................................ True
overlap_param_gather ....................................................... False
override_opt_param_scheduler ................................................ True
padded_vocab_size .......................................................... 32000
param_sync_func ............................................................. None
params_dtype ................................................................ None
patch_dim ..................................................................... 16
perform_initialization ...................................................... True
pipeline_dtype .............................................................. None
pipeline_model_parallel_size ................................................... 1
pipeline_model_parallel_split_rank .......................................... None
position_embedding_type ......................................... learned_absolute
pretrained_checkpoint ....................................................... None
profile .................................................................... False
profile_ranks ................................................................ [0]
profile_step_end .............................................................. 12
profile_step_start ............................................................ 10
qk_layernorm ............................................................... False
query_in_block_prob .......................................................... 0.1
rampup_batch_size ........................................................... None
rank ........................................................................... 0
recompute_activations ...................................................... False
recompute_granularity ....................................................... None
recompute_method ............................................................ None
recompute_num_layers ........................................................ None
reset_attention_mask ....................................................... False
reset_position_ids ......................................................... False
retriever_report_topk_accuracies .............................................. []
retriever_score_scaling .................................................... False
retriever_seq_length ......................................................... 256
retro_add_retriever ........................................................ False
retro_attention_gate ........................................................... 1
retro_cyclic_train_iters .................................................... None
retro_encoder_attention_dropout .............................................. 0.1
retro_encoder_hidden_dropout ................................................. 0.1
retro_encoder_layers ........................................................... 2
retro_num_neighbors ............................................................ 2
retro_num_retrieved_chunks ..................................................... 2
retro_project_dir ........................................................... None
retro_verify_neighbor_count ................................................. True
rotary_base ................................................................ 10000
rotary_interleaved ......................................................... False
rotary_percent ............................................................... 1.0
rotary_seq_len_interpolation_factor ......................................... None
s3_cache_path ............................................................... None
sample_rate .................................................................. 1.0
save ............................................................ ckpt/test_save_1
save_interval .................................................................. 1
scatter_gather_tensors_in_pipeline .......................................... True
seed ........................................................................ 1234
seq_length ................................................................... 512
sequence_parallel .......................................................... False
sgd_momentum ................................................................. 0.9
short_seq_prob ............................................................... 0.1
skip_train ................................................................. False
spec ........................................................................ None
split .................................................................... 100,0,0
squared_relu ............................................................... False
standalone_embedding_stage ................................................. False
start_weight_decay .......................................................... None
straggler_ctrlr_port ....................................................... 65535
straggler_minmax_count ......................................................... 1
swiglu ...................................................................... True
swin_backbone_type .......................................................... tiny
tensor_model_parallel_size ..................................................... 4
tensorboard_dir ............................................................. None
tensorboard_log_interval ....................................................... 1
tensorboard_queue_size ...................................................... 1000
test_data_path .............................................................. None
test_mode .................................................................. False
tiktoken_num_special_tokens ................................................. 1000
tiktoken_pattern ............................................................ None
tiktoken_special_tokens ..................................................... None
timing_log_level ............................................................... 0
timing_log_option ......................................................... minmax
titles_data_path ............................................................ None
tokenizer_model ............................................................. None
tokenizer_type ......................................... GPTSentencePieceTokenizer
tp_comm_bulk_dgrad .......................................................... True
tp_comm_bulk_wgrad .......................................................... True
tp_comm_overlap ............................................................ False
tp_comm_overlap_ag .......................................................... True
tp_comm_overlap_cfg ......................................................... None
tp_comm_overlap_rs .......................................................... True
tp_comm_overlap_rs_dgrad ................................................... False
tp_comm_split_ag ............................................................ True
tp_comm_split_rs ............................................................ True
train_data_path ............................................................. None
train_iters ................................................................... 64
train_samples ............................................................... None
transformer_impl .............................................. transformer_engine
untie_embeddings_and_output_weights ......................................... True
use_checkpoint_args ........................................................ False
use_checkpoint_opt_param_scheduler ......................................... False
use_cpu_initialization ...................................................... None
use_dist_ckpt .............................................................. False
use_distributed_optimizer ................................................... True
use_flash_attn .............................................................. True
use_legacy_models .......................................................... False
use_one_sent_docs .......................................................... False
use_ring_exchange_p2p ...................................................... False
use_rotary_position_embeddings .............................................. True
use_tp_pp_dp_mapping ....................................................... False
valid_data_path ............................................................. None
variable_seq_lengths ....................................................... False
virtual_pipeline_model_parallel_size ........................................ None
vision_backbone_type ......................................................... vit
vision_pretraining ......................................................... False
vision_pretraining_type ................................................. classify
vocab_extra_ids ................................................................ 0
vocab_file .................................................................. None
vocab_size ................................................................. 32000
wandb_exp_name .............................................................. None
wandb_project ............................................................... None
wandb_save_dir .............................................................. None
warmup ...................................................................... None
weight_decay ................................................................. 0.0
weight_decay_incr_style ................................................. constant
wgrad_deferral_limit ........................................................... 0
world_size ..................................................................... 8
yaml_cfg .................................................................... None
--------------------------------------------------------------------------------
[0m
[32m2024-12-22 02:25:19[0m [34m[1mDEBUG   [0m[34mwrap.py:megatron_lm_initialize:101      [0m [34m[1m[STATUS] initializing torch.distributed[0m
[32m2024-12-22 02:25:20[0m [34m[1mDEBUG   [0m[34minitialize.py:_initialize_distributed:231[0m [34m[1m(rank0/8) > initializing torch distributed ...[0m
[32m2024-12-22 02:25:20[0m [34m[1mDEBUG   [0m[34mglobal_vars.py:_set_one_logger:184      [0m [34m[1m(rank7/8) WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it[0m
[32m2024-12-22 02:25:21[0m [34m[1mDEBUG   [0m[34minitialize.py:_initialize_distributed:268[0m [34m[1m(rank0/8) > initialized tensor model parallel with size 4[0m
[32m2024-12-22 02:25:21[0m [34m[1mDEBUG   [0m[34minitialize.py:_initialize_distributed:272[0m [34m[1m(rank0/8) > initialized pipeline model parallel with size 1[0m
[rank4]:[W1222 02:25:21.722079862 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[32m2024-12-22 02:25:21[0m [34m[1mDEBUG   [0m[34mwrap.py:megatron_lm_initialize:104      [0m [34m[1msetting random seeds: 1234[0m
[32m2024-12-22 02:25:21[0m [34m[1mDEBUG   [0m[34mwrap.py:megatron_lm_initialize:107      [0m [34m[1m[STATUS] initializing auto resume[0m
[32m2024-12-22 02:25:21[0m [34m[1mDEBUG   [0m[34mwrap.py:megatron_lm_initialize:119      [0m [34m[1m[STATUS] setting jit fusion options[0m
[rank0]:[W1222 02:25:21.727293346 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank7]:[W1222 02:25:21.968734949 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank3]:[W1222 02:25:21.968758885 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank5]:[W1222 02:25:21.978399027 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank1]:[W1222 02:25:21.981529901 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank6]:[W1222 02:25:21.056357265 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank2]:[W1222 02:25:21.059197326 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[32m2024-12-22 02:25:23[0m [1mINFO    [0m[34mwrap.py:initialize:37                   [0m [1m(rank4/8) [STATUS] initialization finished, parallel state of this rank: (TP0/4 PP0/1 DP1/2 CP0/1 EP0/1)[0m
[32m2024-12-22 02:25:23[0m [1mINFO    [0m[34mwrap.py:initialize:37                   [0m [1m(rank7/8) [STATUS] initialization finished, parallel state of this rank: (TP3/4 PP0/1 DP1/2 CP0/1 EP0/1)[0m
[32m2024-12-22 02:25:23[0m [34m[1mDEBUG   [0m[34mwrap.py:patch_get_parallel_state:158    [0m [34m[1m[PATCH] the series of get parallel state funcs are patched, use (t|p|d|c|e)p_(rank|size) instead of the original to save effort[0m
[32m2024-12-22 02:25:23[0m [1mINFO    [0m[34mwrap.py:initialize:37                   [0m [1m(rank0/8) [STATUS] initialization finished, parallel state of this rank: (TP0/4 PP0/1 DP0/2 CP0/1 EP0/1)[0m
[32m2024-12-22 02:25:23[0m [1mINFO    [0m[34mmodel.py:_model_:29                     [0m [1m[STATUS] building model[0m
[32m2024-12-22 02:25:23[0m [34m[1mDEBUG   [0m[34mmodel.py:_model_:30                     [0m [34m[1muse transformer engine: True, model provider args: ConfigNestNamespace(model_type='GPT', parallel_output=False, show_weight_details=True, encoder_decoder_type='encoder_or_decoder')[0m
[32m2024-12-22 02:25:23[0m [1mINFO    [0m[34mwrap.py:initialize:37                   [0m [1m(rank6/8) [STATUS] initialization finished, parallel state of this rank: (TP2/4 PP0/1 DP1/2 CP0/1 EP0/1)[0m
[32m2024-12-22 02:25:23[0m [1mINFO    [0m[34mwrap.py:initialize:37                   [0m [1m(rank5/8) [STATUS] initialization finished, parallel state of this rank: (TP1/4 PP0/1 DP1/2 CP0/1 EP0/1)[0m
[32m2024-12-22 02:25:23[0m [1mINFO    [0m[34mwrap.py:initialize:37                   [0m [1m(rank1/8) [STATUS] initialization finished, parallel state of this rank: (TP1/4 PP0/1 DP0/2 CP0/1 EP0/1)[0m
[32m2024-12-22 02:25:23[0m [1mINFO    [0m[34mwrap.py:initialize:37                   [0m [1m(rank3/8) [STATUS] initialization finished, parallel state of this rank: (TP3/4 PP0/1 DP0/2 CP0/1 EP0/1)[0m
[32m2024-12-22 02:25:24[0m [34m[1mDEBUG   [0m[34mtraining.py:get_model:429               [0m [34m[1m(rank0/8)  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1684803584[0m
[32m2024-12-22 02:25:24[0m [34m[1mDEBUG   [0m[34mutils.py:print_rank_0:269               [0m [34m[1m(rank0/8) > learning rate decay style: cosine[0m
[32m2024-12-22 02:25:24[0m [1mINFO    [0m[34mwrap.py:initialize:37                   [0m [1m(rank2/8) [STATUS] initialization finished, parallel state of this rank: (TP2/4 PP0/1 DP0/2 CP0/1 EP0/1)[0m
[32m2024-12-22 02:25:24[0m [34m[1mDEBUG   [0m[34mtraining.py:get_model:429               [0m [34m[1m(rank1/8)  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1684803584[0m
[32m2024-12-22 02:25:24[0m [34m[1mDEBUG   [0m[34mtraining.py:get_model:429               [0m [34m[1m(rank3/8)  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1684803584[0m
[32m2024-12-22 02:25:24[0m [34m[1mDEBUG   [0m[34mtraining.py:get_model:429               [0m [34m[1m(rank2/8)  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1684803584[0m
[32m2024-12-22 02:25:24[0m [34m[1mDEBUG   [0m[34mutils.py:print_rank_0:269               [0m [34m[1m(rank0/8)  loading checkpoint from ckpt/llama-2-7b-mcore-tp4pp1 at iteration 1[0m
[32m2024-12-22 02:25:39[0m [34m[1mDEBUG   [0m[34mutils.py:print_rank_0:269               [0m [34m[1m(rank0/8) could not find arguments in the checkpoint ...[0m
[32m2024-12-22 02:25:41[0m [34m[1mDEBUG   [0m[34mutils.py:print_rank_0:269               [0m [34m[1m(rank0/8)  checkpoint version 3.0[0m
[32m2024-12-22 02:25:41[0m [34m[1mDEBUG   [0m[34mutils.py:print_rank_0:269               [0m [34m[1m(rank0/8)   successfully loaded checkpoint from ckpt/llama-2-7b-mcore-tp4pp1 [ t 0, p 0 ] at iteration 0[0m
[32m2024-12-22 02:25:43[0m [1mINFO    [0m[34mwrap.py:setup_model_and_optimizer:211   [0m [1m[STATUS] model is sucessfully built[0m
[32m2024-12-22 02:25:43[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:220   [0m [34m[1m(rank3/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 18.3750
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 52.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.5000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 19.0000
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 92.5000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.0000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.7500
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 24.8750
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.3750
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 86.5000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.3750
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 86.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.8750
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.7500
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.7500
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.0000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.6250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.0000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.8750
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.6250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.5000
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.0000
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.0000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.0000
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.7500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.7500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 70.0000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.5000
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.5000
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 70.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.2500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.5000
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.5000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.5000
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 94.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.5000
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.0000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 100.0000
[0m
[32m2024-12-22 02:25:43[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:220   [0m [34m[1m(rank6/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 99.5000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.4375
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 46.7500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.5000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 18.8750
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 90.0000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.5000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.0000
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 89.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.5000
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 83.0000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 83.0000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.1250
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.3750
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.0000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.1250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.0000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.0000
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.1250
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.6250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.5000
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.7500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.2500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.5000
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.2500
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.7500
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.0000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.0000
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.0000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 72.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.2500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 44.0000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 95.5000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 98.5000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
[0m
[32m2024-12-22 02:25:43[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank3/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:25:43[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:220   [0m [34m[1m(rank5/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 98.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 13.2500
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 46.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.0000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.7500
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 68.0000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 83.5000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 92.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.6250
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.1250
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.3750
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 84.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.2500
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 25.8750
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.8750
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.7500
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.1250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.5000
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.8750
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.0000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.3750
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.2500
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 71.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.7500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 71.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.2500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 41.7500
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.5000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.7500
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 97.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.0000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
[0m
[32m2024-12-22 02:25:43[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:220   [0m [34m[1m(rank1/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 98.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 13.2500
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 46.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.0000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.7500
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 68.0000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 83.5000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 92.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.6250
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.1250
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.3750
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 84.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.2500
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 25.8750
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.8750
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.7500
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.1250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.5000
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.8750
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.0000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.3750
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.2500
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 71.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.7500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 71.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.2500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 41.7500
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.5000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.7500
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 97.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.0000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
[0m
[32m2024-12-22 02:25:43[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank6/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:25:43[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank5/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:25:43[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank1/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:25:43[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:220   [0m [34m[1m(rank2/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 99.5000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.4375
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 46.7500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.5000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 18.8750
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 90.0000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.5000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.0000
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 89.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.5000
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 83.0000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 83.0000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.1250
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.3750
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.0000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.1250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.0000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.0000
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.1250
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.6250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.5000
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.7500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.2500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.5000
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.2500
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.7500
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.0000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.0000
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.0000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 72.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.2500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 44.0000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 95.5000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 98.5000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
[0m
[32m2024-12-22 02:25:43[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank2/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:25:43[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:220   [0m [34m[1m(rank4/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 92.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 11.8750
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 43.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.0000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.3125
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 62.2500
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.0000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.5000
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.8750
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.8750
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.6250
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.0000
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.0000
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.2500
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.1250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.7500
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.5000
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.0000
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.7500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.6250
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.5000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.2500
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.7500
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.0000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.2500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.0000
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 72.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.7500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.5000
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 94.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.7500
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.0000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.5000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 98.0000
[0m
[32m2024-12-22 02:25:43[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank4/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:25:43[0m [34m[1mDEBUG   [0m[34mtimers.py:log:369                       [0m [34m[1m(rank7/8) (min, max) time across ranks (ms):
    load-checkpoint ................................: (18004.49, 18004.91)[0m
[32m2024-12-22 02:25:43[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:220   [0m [34m[1m(rank0/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 92.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 11.8750
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 43.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.0000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.3125
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 62.2500
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.0000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.5000
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.8750
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.8750
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.6250
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.0000
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.0000
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.2500
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.1250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.7500
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.5000
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.0000
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.7500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.6250
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.5000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.2500
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.7500
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.0000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.2500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.0000
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 72.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.7500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.5000
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 94.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.7500
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.0000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.5000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 98.0000
[0m
[32m2024-12-22 02:25:43[0m [1mINFO    [0m[34mwrap.py:setup_model_and_optimizer:224   [0m [1m[STATUS] optimizer is sucessfully built: DistributedOptimizer(type=adam, lr=2e-05, min_lr=0.0, weight_decay=0.0adam_beta=(0.9,0.95), adam_eps=1e-08, sgd_momentum=0.9[0m
[32m2024-12-22 02:25:43[0m [1mINFO    [0m[34mwrap.py:setup_model_and_optimizer:225   [0m [1m[STATUS] scheduler is sucessfully built: OptimizerParamScheduler(lr_decay_style=cosine, lr_warmup_steps=409.6, lr_decay_steps=8192)[0m
[32m2024-12-22 02:25:43[0m [1mINFO    [0m[34mwrap.py:_set_megatron_wrap_training_flow:284[0m [1m[STATUS] successfully set megatron wrap training flow: MegatronWrapMinimalMockMSEFlow(flow_key=minimal_mock_mse, seq_length=512, micro_batch_size=4)[0m
[32m2024-12-22 02:25:43[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank0/8) [WRAP] split batch data with dp, DP0/2 got 64 of 128[0m
[32m2024-12-22 02:25:43[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:220   [0m [34m[1m(rank7/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 18.3750
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 52.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.5000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 19.0000
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 92.5000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.0000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.7500
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 24.8750
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.3750
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 86.5000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.3750
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 86.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.8750
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.7500
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.7500
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.0000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.6250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.0000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.8750
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.6250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.5000
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.0000
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.0000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.0000
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.7500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.7500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 70.0000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.5000
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.5000
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 70.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.2500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.5000
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.5000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.5000
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 94.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.5000
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.0000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 100.0000
[0m
[32m2024-12-22 02:25:43[0m [34m[1mDEBUG   [0m[34mwrap.py:train:303                       [0m [34m[1m(rank7/8) [WRAP] split batch data with dp, DP1/2 got 64 of 128[0m
[32m2024-12-22 02:25:45[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 0) loss: 80.0595, random_length: 452.0000[0m
[32m2024-12-22 02:25:48[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 1) loss: 109.5258, random_length: 120.0000[0m
[32m2024-12-22 02:25:50[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 2) loss: 114.4262, random_length: 8.0000[0m
[32m2024-12-22 02:25:52[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 3) loss: 107.2352, random_length: 93.0000[0m
[32m2024-12-22 02:25:53[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 4) loss: 136.8486, random_length: 36.0000[0m
[32m2024-12-22 02:25:55[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 5) loss: 103.2843, random_length: 86.0000[0m
[32m2024-12-22 02:25:57[0m [34m[1mDEBUG   [0m[34mflow.py:log_each_step_metrics:48        [0m [34m[1m(micro batch step 6) loss: 108.6465, random_length: 101.0000[0m
