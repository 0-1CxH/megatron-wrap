[32m2024-12-21 10:36:21[0m [34m[1mDEBUG   [0m[34mdist_logger.py:patch_logger:48          [0m [34m[1m[PATCH] logger patched, use (error|warning|info|debug)_(rank_0|all_ranks) instead of the original to avoid unexpected behavior[0m
[32m2024-12-21 10:36:22[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:46   [0m [34m[1m(rank7/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-21 10:36:22[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:46   [0m [34m[1m(rank6/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-21 10:36:23[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:46   [0m [34m[1m(rank5/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-21 10:36:23[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:46   [0m [34m[1m(rank3/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-21 10:36:23[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:46   [0m [34m[1m(rank4/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-21 10:36:23[0m [34m[1mDEBUG   [0m[34mconfig.py:check_args_compatibility:58   [0m [34m[1mpadded_vocab_size is set to  32000[0m
[32m2024-12-21 10:36:23[0m [34m[1mDEBUG   [0m[34mwrap.py:handle_display:54               [0m [34m[1m[PATCH] python builtin print patched, all print() will go to debug_all_ranks[0m
[32m2024-12-21 10:36:23[0m [34m[1mDEBUG   [0m[34mwrap.py:handle_display:60               [0m [34m[1m[PATCH] all logging handlers are removed, logging funcs no longer logs anything[0m
[32m2024-12-21 10:36:23[0m [34m[1mDEBUG   [0m[34mwrap.py:handle_display:65               [0m [34m[1m[PATCH] FutureWarning, UserWarning are ignored[0m
[32m2024-12-21 10:36:23[0m [1mINFO    [0m[34mwrap.py:initialize:30                   [0m [1m[STATUS] initialization started[0m
[32m2024-12-21 10:36:23[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:42   [0m [34m[1m[WRAP] added '/gpfs/public/infra/qhu/projects/megatron_core_080' to sys.path[0m
[32m2024-12-21 10:36:23[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:46   [0m [34m[1m(rank0/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-21 10:36:23[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:46   [0m [34m[1m(rank2/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-21 10:36:23[0m [34m[1mDEBUG   [0m[34mwrap.py:dynamic_import_magatron_lm:46   [0m [34m[1m(rank1/8) [STATUS] megatron-lm imported successfully[0m
[32m2024-12-21 10:36:28[0m [34m[1mDEBUG   [0m[34mglobal_vars.py:_set_one_logger:184      [0m [34m[1m(rank7/8) WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it[0m
[32m2024-12-21 10:36:29[0m [34m[1mDEBUG   [0m[34marguments.py:validate_args:184          [0m [34m[1m(rank0/8) using world size: 8, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 4, pipeline-model-parallel size: 1 [0m
[32m2024-12-21 10:36:29[0m [34m[1mDEBUG   [0m[34marguments.py:validate_args:276          [0m [34m[1m(rank0/8) WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication[0m
[32m2024-12-21 10:36:29[0m [34m[1mDEBUG   [0m[34marguments.py:validate_args:307          [0m [34m[1m(rank0/8) accumulate and all-reduce gradients in fp32 for bfloat16 data type.[0m
[32m2024-12-21 10:36:29[0m [34m[1mDEBUG   [0m[34marguments.py:validate_args:311          [0m [34m[1m(rank0/8) using torch.bfloat16 for parameters ...[0m
[32m2024-12-21 10:36:29[0m [1mINFO    [0m[34mwrap.py:<lambda>:73                     [0m [1m
---------------------------- megatron-lm arguments ----------------------------
accumulate_allreduce_grads_in_fp32 ......................................... False
adam_beta1 ................................................................... 0.9
adam_beta2 .................................................................. 0.95
adam_eps ................................................................... 1e-08
add_bias_linear ............................................................ False
add_position_embedding ..................................................... False
add_qkv_bias ............................................................... False
adlr_autoresume ............................................................ False
adlr_autoresume_interval .................................................... 1000
app_tag_run_name ............................................................ None
app_tag_run_version ........................................................ 0.0.0
apply_layernorm_1p ......................................................... False
apply_query_key_layer_scaling .............................................. False
apply_residual_connection_post_layernorm ................................... False
apply_rope_fusion ........................................................... True
async_save .................................................................. None
async_tensor_model_parallel_allreduce ....................................... True
attention_dropout ............................................................ 0.0
attention_softmax_in_fp32 ................................................... True
auto_detect_ckpt_format .................................................... False
barrier_with_L1_time ........................................................ True
batch_size .................................................................. None
bert_binary_head ............................................................ True
bert_embedder_type ...................................................... megatron
bert_load ................................................................... None
bf16 ........................................................................ True
bias_activation_fusion ..................................................... False
bias_dropout_fusion ......................................................... True
bias_gelu_fusion ............................................................ True
bias_swiglu_fusion .......................................................... True
biencoder_projection_dim ....................................................... 0
biencoder_shared_query_context_model ....................................... False
block_data_path ............................................................. None
calculate_per_token_loss ................................................... False
check_for_nan_in_loss_and_grad .............................................. True
check_weight_hash_across_dp_replicas_interval ............................... None
checkpoint_activations ...................................................... None
ckpt_assume_constant_structure ............................................. False
ckpt_fully_parallel_load ................................................... False
ckpt_fully_parallel_save .................................................... True
ckpt_fully_parallel_save_deprecated ........................................ False
ckpt_step ................................................................... None
classes_fraction ............................................................. 1.0
clip_grad .................................................................... 1.0
clone_scatter_output_in_embedding ........................................... True
consumed_train_samples ......................................................... 0
consumed_valid_samples ......................................................... 0
context_parallel_size .......................................................... 1
create_attention_mask_in_dataloader ......................................... True
cross_entropy_loss_fusion .................................................. False
data_cache_path ............................................................. None
data_parallel_random_init .................................................. False
data_parallel_size .......................................................... None
data_path ................................................................... None
data_per_class_fraction ...................................................... 1.0
data_sharding ............................................................... True
dataloader_type ........................................................... cyclic
ddp_average_in_collective .................................................. False
ddp_bucket_size ............................................................. None
decoder_num_layers .......................................................... None
decoder_seq_length .......................................................... None
decoupled_lr ................................................................ None
decoupled_min_lr ............................................................ None
defer_embedding_wgrad_compute .............................................. False
delay_grad_reduce ........................................................... True
delay_param_gather ......................................................... False
deterministic_mode ......................................................... False
dino_bottleneck_size ......................................................... 256
dino_freeze_last_layer ......................................................... 1
dino_head_hidden_size ....................................................... 2048
dino_local_crops_number ....................................................... 10
dino_local_img_size ........................................................... 96
dino_norm_last_layer ....................................................... False
dino_teacher_temp ........................................................... 0.07
dino_warmup_teacher_temp .................................................... 0.04
dino_warmup_teacher_temp_epochs ............................................... 30
disable_straggler_on_startup ............................................... False
dist_ckpt_format ...................................................... torch_dist
dist_ckpt_strictness ........................................ assume_ok_unexpected
distribute_saved_activations ............................................... False
distributed_backend ......................................................... nccl
distributed_timeout_minutes ................................................... 60
embedding_path .............................................................. None
empty_unused_memory_level ...................................................... 0
enable_one_logger ........................................................... True
encoder_num_layers .......................................................... None
encoder_seq_length .......................................................... None
end_weight_decay ............................................................ None
eod_mask_loss .............................................................. False
eval_interval ............................................................... None
eval_iters .................................................................. None
evidence_data_path .......................................................... None
exit_duration_in_mins ....................................................... None
exit_interval ............................................................... None
exit_on_missing_checkpoint .................................................. True
exit_signal_handler ........................................................ False
expert_model_parallel_size ..................................................... 1
ffn_hidden_size ............................................................ 11008
finetune .................................................................... True
fp16 ....................................................................... False
fp16_lm_cross_entropy ...................................................... False
fp32_residual_connection ................................................... False
fp8 ......................................................................... None
fp8_amax_compute_algo ........................................................ max
fp8_amax_history_len ........................................................ 1024
fp8_interval ................................................................... 1
fp8_margin ..................................................................... 0
fp8_wgrad ................................................................... True
gated_linear_unit .......................................................... False
global_batch_size ............................................................ 128
gradient_accumulation_fusion ................................................ True
group_query_attention ...................................................... False
head_lr_mult ................................................................. 1.0
hidden_dropout ............................................................... 0.0
hidden_size ................................................................. 4096
hybrid_attention_ratio ....................................................... 0.0
hybrid_mlp_ratio ............................................................. 0.0
hybrid_override_pattern ..................................................... None
hysteresis ..................................................................... 2
ict_head_size ............................................................... None
ict_load .................................................................... None
img_h ........................................................................ 224
img_w ........................................................................ 224
indexer_batch_size ........................................................... 128
indexer_log_interval ........................................................ 1000
inference_batch_times_seqlen_threshold ....................................... 512
init_method_std ............................................................ 0.006
init_method_xavier_uniform ................................................. False
initial_loss_scale .................................................... 4294967296
iter_per_epoch .............................................................. 1250
kv_channels ................................................................. None
lazy_mpu_init ............................................................... None
load ................................................ ckpt/llama-2-7b-mcore-tp4pp1
local_rank ..................................................................... 0
log_batch_size_to_tensorboard ............................................... True
log_interval ................................................................... 1
log_learning_rate_to_tensorboard ............................................ True
log_loss_scale_to_tensorboard ............................................... True
log_memory_to_tensorboard .................................................. False
log_num_zeros_in_grad ....................................................... True
log_params_norm ............................................................. True
log_progress ............................................................... False
log_straggler .............................................................. False
log_throughput .............................................................. True
log_timers_to_tensorboard ................................................... True
log_validation_ppl_to_tensorboard .......................................... False
log_world_size_to_tensorboard .............................................. False
logging_level ............................................................... None
loss_scale .................................................................. None
loss_scale_window ........................................................... 1000
lr ......................................................................... 2e-05
lr_decay_iters .............................................................. None
lr_decay_samples ............................................................ None
lr_decay_style ............................................................ cosine
lr_warmup_fraction .......................................................... 0.05
lr_warmup_init ............................................................... 0.0
lr_warmup_iters ................................................................ 0
lr_warmup_samples .............................................................. 0
lr_wsd_decay_iters .......................................................... None
lr_wsd_decay_samples ........................................................ None
lr_wsd_decay_style ................................................... exponential
make_vocab_size_divisible_by ................................................. 100
manual_gc .................................................................. False
manual_gc_eval .............................................................. True
manual_gc_interval ............................................................. 0
mask_factor .................................................................. 1.0
mask_prob ................................................................... 0.15
mask_type ................................................................. random
masked_softmax_fusion ...................................................... False
max_position_embeddings ..................................................... 4096
max_tokens_to_oom .......................................................... 12000
merge_file .................................................................. None
micro_batch_size ............................................................... 4
min_loss_scale ............................................................... 1.0
min_lr ....................................................................... 0.0
mmap_bin_files .............................................................. True
mock_data .................................................................. False
model_parallel_size ......................................................... None
moe_aux_loss_coeff ............................................................. 0
moe_expert_capacity_factor .................................................. None
moe_extended_tp ............................................................ False
moe_grouped_gemm ........................................................... False
moe_input_jitter_eps ........................................................ None
moe_layer_recompute ........................................................ False
moe_pad_expert_input_to_capacity ........................................... False
moe_per_layer_logging ...................................................... False
moe_router_load_balancing_type .......................................... aux_loss
moe_router_pre_softmax ..................................................... False
moe_router_topk ................................................................ 2
moe_token_dispatcher_type .............................................. allgather
moe_token_drop_policy ...................................................... probs
moe_z_loss_coeff ............................................................ None
nccl_communicator_config_path ............................................... None
no_load_optim ............................................................... True
no_load_rng ................................................................. True
no_persist_layer_norm ...................................................... False
no_save_optim ............................................................... True
no_save_rng ................................................................. True
no_sync_func ................................................................ None
norm_epsilon ............................................................... 1e-05
normalization ............................................................ RMSNorm
num_attention_heads ........................................................... 32
num_channels ................................................................... 3
num_classes ................................................................. 1000
num_dataset_builder_threads .................................................... 1
num_experts ................................................................. None
num_layers .................................................................... 32
num_layers_per_virtual_pipeline_stage ....................................... None
num_microbatches_with_partial_activation_checkpoints ........................ None
num_moe_experts ............................................................. None
num_query_groups ............................................................ None
num_workers .................................................................... 8
one_logger_async ........................................................... False
one_logger_project ................................................... megatron-lm
one_logger_run_name ......................................................... None
onnx_safe ................................................................... None
openai_gelu ................................................................ False
optimizer ................................................................... adam
output_bert_embeddings ..................................................... False
overlap_grad_reduce ........................................................ False
overlap_p2p_comm ............................................................ True
overlap_param_gather ....................................................... False
override_opt_param_scheduler ................................................ True
padded_vocab_size .......................................................... 32000
param_sync_func ............................................................. None
params_dtype ................................................................ None
patch_dim ..................................................................... 16
perform_initialization ...................................................... True
pipeline_dtype .............................................................. None
pipeline_model_parallel_size ................................................... 1
pipeline_model_parallel_split_rank .......................................... None
position_embedding_type ......................................... learned_absolute
pretrained_checkpoint ....................................................... None
profile .................................................................... False
profile_ranks ................................................................ [0]
profile_step_end .............................................................. 12
profile_step_start ............................................................ 10
qk_layernorm ............................................................... False
query_in_block_prob .......................................................... 0.1
rampup_batch_size ........................................................... None
rank ........................................................................... 0
recompute_activations ...................................................... False
recompute_granularity ....................................................... None
recompute_method ............................................................ None
recompute_num_layers ........................................................ None
reset_attention_mask ....................................................... False
reset_position_ids ......................................................... False
retriever_report_topk_accuracies .............................................. []
retriever_score_scaling .................................................... False
retriever_seq_length ......................................................... 256
retro_add_retriever ........................................................ False
retro_attention_gate ........................................................... 1
retro_cyclic_train_iters .................................................... None
retro_encoder_attention_dropout .............................................. 0.1
retro_encoder_hidden_dropout ................................................. 0.1
retro_encoder_layers ........................................................... 2
retro_num_neighbors ............................................................ 2
retro_num_retrieved_chunks ..................................................... 2
retro_project_dir ........................................................... None
retro_verify_neighbor_count ................................................. True
rotary_base ................................................................ 10000
rotary_interleaved ......................................................... False
rotary_percent ............................................................... 1.0
rotary_seq_len_interpolation_factor ......................................... None
s3_cache_path ............................................................... None
sample_rate .................................................................. 1.0
save ............................................................ ckpt/test_save_1
save_interval .................................................................. 1
scatter_gather_tensors_in_pipeline .......................................... True
seed ........................................................................ 1234
seq_length ................................................................... 512
sequence_parallel ........................................................... True
sgd_momentum ................................................................. 0.9
short_seq_prob ............................................................... 0.1
skip_train ................................................................. False
spec ........................................................................ None
split .................................................................... 100,0,0
squared_relu ............................................................... False
standalone_embedding_stage ................................................. False
start_weight_decay .......................................................... None
straggler_ctrlr_port ....................................................... 65535
straggler_minmax_count ......................................................... 1
swiglu ...................................................................... True
swin_backbone_type .......................................................... tiny
tensor_model_parallel_size ..................................................... 4
tensorboard_dir ............................................................. None
tensorboard_log_interval ....................................................... 1
tensorboard_queue_size ...................................................... 1000
test_data_path .............................................................. None
test_mode .................................................................. False
tiktoken_num_special_tokens ................................................. 1000
tiktoken_pattern ............................................................ None
tiktoken_special_tokens ..................................................... None
timing_log_level ............................................................... 0
timing_log_option ......................................................... minmax
titles_data_path ............................................................ None
tokenizer_model ............................................................. None
tokenizer_type ......................................... GPTSentencePieceTokenizer
tp_comm_bulk_dgrad .......................................................... True
tp_comm_bulk_wgrad .......................................................... True
tp_comm_overlap ............................................................ False
tp_comm_overlap_ag .......................................................... True
tp_comm_overlap_cfg ......................................................... None
tp_comm_overlap_rs .......................................................... True
tp_comm_overlap_rs_dgrad ................................................... False
tp_comm_split_ag ............................................................ True
tp_comm_split_rs ............................................................ True
train_data_path ............................................................. None
train_iters ................................................................... 64
train_samples ............................................................... None
transformer_impl .............................................. transformer_engine
untie_embeddings_and_output_weights ......................................... True
use_checkpoint_args ........................................................ False
use_checkpoint_opt_param_scheduler ......................................... False
use_cpu_initialization ...................................................... None
use_dist_ckpt .............................................................. False
use_distributed_optimizer ................................................... True
use_flash_attn .............................................................. True
use_legacy_models .......................................................... False
use_one_sent_docs .......................................................... False
use_ring_exchange_p2p ...................................................... False
use_rotary_position_embeddings .............................................. True
use_tp_pp_dp_mapping ....................................................... False
valid_data_path ............................................................. None
variable_seq_lengths ....................................................... False
virtual_pipeline_model_parallel_size ........................................ None
vision_backbone_type ......................................................... vit
vision_pretraining ......................................................... False
vision_pretraining_type ................................................. classify
vocab_extra_ids ................................................................ 0
vocab_file .................................................................. None
vocab_size ................................................................. 32000
wandb_exp_name .............................................................. None
wandb_project ............................................................... None
wandb_save_dir .............................................................. None
warmup ...................................................................... None
weight_decay ................................................................. 0.0
weight_decay_incr_style ................................................. constant
wgrad_deferral_limit ........................................................... 0
world_size ..................................................................... 8
yaml_cfg .................................................................... None
--------------------------------------------------------------------------------
[0m
[32m2024-12-21 10:36:29[0m [34m[1mDEBUG   [0m[34mwrap.py:megatron_lm_initialize:100      [0m [34m[1m[STATUS] initializing torch.distributed[0m
[rank3]:[W1221 22:36:30.746055277 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[32m2024-12-21 10:36:30[0m [34m[1mDEBUG   [0m[34minitialize.py:_initialize_distributed:231[0m [34m[1m(rank0/8) > initializing torch distributed ...[0m
[rank7]:[W1221 22:36:30.747150825 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank5]:[W1221 22:36:30.913181786 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank1]:[W1221 22:36:30.914234419 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank2]:[W1221 22:36:30.948052388 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank6]:[W1221 22:36:30.950757293 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[32m2024-12-21 10:36:30[0m [34m[1mDEBUG   [0m[34minitialize.py:_initialize_distributed:268[0m [34m[1m(rank0/8) > initialized tensor model parallel with size 4[0m
[32m2024-12-21 10:36:30[0m [34m[1mDEBUG   [0m[34minitialize.py:_initialize_distributed:272[0m [34m[1m(rank0/8) > initialized pipeline model parallel with size 1[0m
[32m2024-12-21 10:36:30[0m [34m[1mDEBUG   [0m[34mwrap.py:megatron_lm_initialize:103      [0m [34m[1msetting random seeds: 1234[0m
[32m2024-12-21 10:36:30[0m [34m[1mDEBUG   [0m[34mwrap.py:megatron_lm_initialize:106      [0m [34m[1m[STATUS] initializing auto resume[0m
[32m2024-12-21 10:36:30[0m [34m[1mDEBUG   [0m[34mwrap.py:megatron_lm_initialize:118      [0m [34m[1m[STATUS] setting jit fusion options[0m
[rank0]:[W1221 22:36:30.964434737 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank4]:[W1221 22:36:30.966603193 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[32m2024-12-21 10:36:32[0m [1mINFO    [0m[34mwrap.py:initialize:36                   [0m [1m(rank6/8) [STATUS] initialization finished, parallel state of this rank: (TP2/4 PP0/1 DP1/2 CP0/1 EP0/1)[0m
[32m2024-12-21 10:36:32[0m [1mINFO    [0m[34mwrap.py:initialize:36                   [0m [1m(rank2/8) [STATUS] initialization finished, parallel state of this rank: (TP2/4 PP0/1 DP0/2 CP0/1 EP0/1)[0m
[32m2024-12-21 10:36:32[0m [1mINFO    [0m[34mwrap.py:initialize:36                   [0m [1m(rank3/8) [STATUS] initialization finished, parallel state of this rank: (TP3/4 PP0/1 DP0/2 CP0/1 EP0/1)[0m
[32m2024-12-21 10:36:32[0m [1mINFO    [0m[34mwrap.py:initialize:36                   [0m [1m(rank1/8) [STATUS] initialization finished, parallel state of this rank: (TP1/4 PP0/1 DP0/2 CP0/1 EP0/1)[0m
[32m2024-12-21 10:36:32[0m [1mINFO    [0m[34mwrap.py:initialize:36                   [0m [1m(rank7/8) [STATUS] initialization finished, parallel state of this rank: (TP3/4 PP0/1 DP1/2 CP0/1 EP0/1)[0m
[32m2024-12-21 10:36:32[0m [1mINFO    [0m[34mwrap.py:initialize:36                   [0m [1m(rank5/8) [STATUS] initialization finished, parallel state of this rank: (TP1/4 PP0/1 DP1/2 CP0/1 EP0/1)[0m
[32m2024-12-21 10:36:32[0m [34m[1mDEBUG   [0m[34mwrap.py:patch_get_parallel_state:157    [0m [34m[1m[PATCH] the series of get parallel state funcs are patched, use (t|p|d|c|e)p_(rank|size) instead of the original to save effort[0m
[32m2024-12-21 10:36:32[0m [1mINFO    [0m[34mwrap.py:initialize:36                   [0m [1m(rank0/8) [STATUS] initialization finished, parallel state of this rank: (TP0/4 PP0/1 DP0/2 CP0/1 EP0/1)[0m
[32m2024-12-21 10:36:32[0m [1mINFO    [0m[34mmodel.py:_model_:29                     [0m [1m[STATUS] building model[0m
[32m2024-12-21 10:36:32[0m [34m[1mDEBUG   [0m[34mmodel.py:_model_:30                     [0m [34m[1muse transformer engine: True, model provider args: ConfigNestNamespace(model_type='GPT', parallel_output=False, show_weight_details=True, encoder_decoder_type='encoder_or_decoder')[0m
[32m2024-12-21 10:36:32[0m [34m[1mDEBUG   [0m[34mtraining.py:get_model:429               [0m [34m[1m(rank2/8)  > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 1684803584[0m
[32m2024-12-21 10:36:32[0m [1mINFO    [0m[34mwrap.py:initialize:36                   [0m [1m(rank4/8) [STATUS] initialization finished, parallel state of this rank: (TP0/4 PP0/1 DP1/2 CP0/1 EP0/1)[0m
[32m2024-12-21 10:36:33[0m [34m[1mDEBUG   [0m[34mtraining.py:get_model:429               [0m [34m[1m(rank3/8)  > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 1684803584[0m
[32m2024-12-21 10:36:33[0m [34m[1mDEBUG   [0m[34mtraining.py:get_model:429               [0m [34m[1m(rank1/8)  > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1684803584[0m
[32m2024-12-21 10:36:33[0m [34m[1mDEBUG   [0m[34mtraining.py:get_model:429               [0m [34m[1m(rank0/8)  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1684803584[0m
[32m2024-12-21 10:36:33[0m [34m[1mDEBUG   [0m[34mutils.py:print_rank_0:269               [0m [34m[1m(rank0/8) > learning rate decay style: cosine[0m
[32m2024-12-21 10:36:33[0m [34m[1mDEBUG   [0m[34mutils.py:print_rank_0:269               [0m [34m[1m(rank0/8)  loading checkpoint from ckpt/llama-2-7b-mcore-tp4pp1 at iteration 1[0m
[32m2024-12-21 10:36:47[0m [34m[1mDEBUG   [0m[34mutils.py:print_rank_0:269               [0m [34m[1m(rank0/8) could not find arguments in the checkpoint ...[0m
[32m2024-12-21 10:36:49[0m [34m[1mDEBUG   [0m[34mutils.py:print_rank_0:269               [0m [34m[1m(rank0/8)  checkpoint version 3.0[0m
[32m2024-12-21 10:36:49[0m [34m[1mDEBUG   [0m[34mutils.py:print_rank_0:269               [0m [34m[1m(rank0/8)   successfully loaded checkpoint from ckpt/llama-2-7b-mcore-tp4pp1 [ t 0, p 0 ] at iteration 0[0m
[32m2024-12-21 10:36:50[0m [1mINFO    [0m[34mwrap.py:setup_model_and_optimizer:210   [0m [1m[STATUS] model is sucessfully built[0m
[32m2024-12-21 10:36:50[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:219   [0m [34m[1m(rank3/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 18.3750
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 52.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.5000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 19.0000
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 92.5000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.0000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.7500
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 24.8750
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.3750
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 86.5000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.3750
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 86.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.8750
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.7500
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.7500
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.0000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.6250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.0000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.8750
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.6250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.5000
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.0000
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.0000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.0000
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.7500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.7500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 70.0000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.5000
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.5000
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 70.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.2500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.5000
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.5000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.5000
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 94.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.5000
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.0000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 100.0000
[0m
[32m2024-12-21 10:36:50[0m [1mINFO    [0m[34mmain.py:<module>:8                      [0m [1m(rank3/8) namespace(tp_size=4, pp_size=1, dp_size=2, cp_size=1, ep_size=1, tp_rank=3, pp_rank=0, dp_rank=0, cp_rank=0, ep_rank=0)[0m
[32m2024-12-21 10:36:50[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:219   [0m [34m[1m(rank2/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 99.5000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.4375
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 46.7500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.5000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 18.8750
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 90.0000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.5000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.0000
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 89.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.5000
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 83.0000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 83.0000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.1250
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.3750
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.0000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.1250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.0000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.0000
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.1250
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.6250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.5000
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.7500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.2500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.5000
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.2500
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.7500
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.0000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.0000
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.0000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 72.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.2500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 44.0000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 95.5000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 98.5000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
[0m
[32m2024-12-21 10:36:50[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:219   [0m [34m[1m(rank6/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 99.5000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.4375
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 46.7500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.5000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 18.8750
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 90.0000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.5000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.0000
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 89.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.5000
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 83.0000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 83.0000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.1250
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.5000
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.3750
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.0000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.1250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.0000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.0000
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.1250
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.6250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.5000
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.7500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.2500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.5000
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.2500
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.7500
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.0000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.0000
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.2500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.0000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 72.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.2500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 44.0000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 95.5000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 98.5000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
[0m
[32m2024-12-21 10:36:50[0m [1mINFO    [0m[34mmain.py:<module>:8                      [0m [1m(rank2/8) namespace(tp_size=4, pp_size=1, dp_size=2, cp_size=1, ep_size=1, tp_rank=2, pp_rank=0, dp_rank=0, cp_rank=0, ep_rank=0)[0m
[32m2024-12-21 10:36:50[0m [1mINFO    [0m[34mmain.py:<module>:8                      [0m [1m(rank6/8) namespace(tp_size=4, pp_size=1, dp_size=2, cp_size=1, ep_size=1, tp_rank=2, pp_rank=0, dp_rank=1, cp_rank=0, ep_rank=0)[0m
[32m2024-12-21 10:36:50[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:219   [0m [34m[1m(rank5/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 98.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 13.2500
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 46.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.0000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.7500
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 68.0000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 83.5000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 92.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.6250
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.1250
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.3750
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 84.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.2500
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 25.8750
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.8750
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.7500
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.1250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.5000
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.8750
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.0000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.3750
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.2500
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 71.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.7500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 71.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.2500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 41.7500
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.5000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.7500
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 97.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.0000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
[0m
[32m2024-12-21 10:36:50[0m [1mINFO    [0m[34mmain.py:<module>:8                      [0m [1m(rank5/8) namespace(tp_size=4, pp_size=1, dp_size=2, cp_size=1, ep_size=1, tp_rank=1, pp_rank=0, dp_rank=1, cp_rank=0, ep_rank=0)[0m
[32m2024-12-21 10:36:50[0m [34m[1mDEBUG   [0m[34mwrap.py:train:300                       [0m [34m[1m(rank3/8) [WRAP] split dp, DP0/2 got 64 of 128[0m
[32m2024-12-21 10:36:50[0m [34m[1mDEBUG   [0m[34mwrap.py:train:300                       [0m [34m[1m(rank2/8) [WRAP] split dp, DP0/2 got 64 of 128[0m
[32m2024-12-21 10:36:50[0m [34m[1mDEBUG   [0m[34mwrap.py:train:300                       [0m [34m[1m(rank6/8) [WRAP] split dp, DP1/2 got 64 of 128[0m
[32m2024-12-21 10:36:50[0m [34m[1mDEBUG   [0m[34mwrap.py:train:300                       [0m [34m[1m(rank5/8) [WRAP] split dp, DP1/2 got 64 of 128[0m
[32m2024-12-21 10:36:50[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:219   [0m [34m[1m(rank4/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 92.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 11.8750
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 43.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.0000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.3125
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 62.2500
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.0000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.5000
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.8750
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.8750
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.6250
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.0000
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.0000
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.2500
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.1250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.7500
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.5000
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.0000
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.7500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.6250
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.5000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.2500
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.7500
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.0000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.2500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.0000
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 72.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.7500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.5000
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 94.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.7500
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.0000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.5000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 98.0000
[0m
[32m2024-12-21 10:36:50[0m [1mINFO    [0m[34mmain.py:<module>:8                      [0m [1m(rank4/8) namespace(tp_size=4, pp_size=1, dp_size=2, cp_size=1, ep_size=1, tp_rank=0, pp_rank=0, dp_rank=1, cp_rank=0, ep_rank=0)[0m
[32m2024-12-21 10:36:51[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:219   [0m [34m[1m(rank1/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 98.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 13.2500
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 46.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.0000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.7500
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 68.0000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 83.5000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 92.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.6250
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.1250
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.3750
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 84.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.2500
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 25.8750
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.8750
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.7500
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.1250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.5000
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.8750
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.0000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.3750
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.2500
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 71.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.7500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 71.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.2500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.0000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 41.7500
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.5000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.7500
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 97.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.0000
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.0000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
[0m
[32m2024-12-21 10:36:51[0m [1mINFO    [0m[34mmain.py:<module>:8                      [0m [1m(rank1/8) namespace(tp_size=4, pp_size=1, dp_size=2, cp_size=1, ep_size=1, tp_rank=1, pp_rank=0, dp_rank=0, cp_rank=0, ep_rank=0)[0m
[32m2024-12-21 10:36:51[0m [34m[1mDEBUG   [0m[34mwrap.py:train:300                       [0m [34m[1m(rank4/8) [WRAP] split dp, DP1/2 got 64 of 128[0m
[rank3]: Traceback (most recent call last):
[rank3]:   File "/gpfs/public/infra/qhu/projects/megatron-wrap/main.py", line 9, in <module>
[rank3]:     c.train([[]]*128)
[rank3]:   File "/gpfs/public/infra/qhu/projects/megatron-wrap/megatron_wrap/core/wrap.py", line 303, in train
[rank3]:     train_step(self.megatron_wrap_training_flow.forward_func,
[rank3]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/training/training.py", line 590, in train_step
[rank3]:     losses_reduced = forward_backward_func(
[rank3]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/core/pipeline_parallel/schedules.py", line 439, in forward_backward_no_pipelining
[rank3]:     output_tensor, num_tokens = forward_step(
[rank3]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/core/pipeline_parallel/schedules.py", line 264, in forward_step
[rank3]:     output_tensor, loss_func = forward_step_func(data_iterator, model)
[rank3]: TypeError: MegatronWrapTrainingFlowBase.forward_func() takes 2 positional arguments but 3 were given
[rank2]: Traceback (most recent call last):
[rank2]:   File "/gpfs/public/infra/qhu/projects/megatron-wrap/main.py", line 9, in <module>
[rank2]:     c.train([[]]*128)
[rank2]:   File "/gpfs/public/infra/qhu/projects/megatron-wrap/megatron_wrap/core/wrap.py", line 303, in train
[rank2]:     train_step(self.megatron_wrap_training_flow.forward_func,
[rank2]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/training/training.py", line 590, in train_step
[rank2]:     losses_reduced = forward_backward_func(
[rank2]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/core/pipeline_parallel/schedules.py", line 439, in forward_backward_no_pipelining
[rank2]:     output_tensor, num_tokens = forward_step(
[rank2]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/core/pipeline_parallel/schedules.py", line 264, in forward_step
[rank2]:     output_tensor, loss_func = forward_step_func(data_iterator, model)
[rank2]: TypeError: MegatronWrapTrainingFlowBase.forward_func() takes 2 positional arguments but 3 were given
[rank6]: Traceback (most recent call last):
[rank6]:   File "/gpfs/public/infra/qhu/projects/megatron-wrap/main.py", line 9, in <module>
[rank6]:     c.train([[]]*128)
[rank6]:   File "/gpfs/public/infra/qhu/projects/megatron-wrap/megatron_wrap/core/wrap.py", line 303, in train
[rank6]:     train_step(self.megatron_wrap_training_flow.forward_func,
[rank6]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/training/training.py", line 590, in train_step
[rank6]:     losses_reduced = forward_backward_func(
[rank6]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/core/pipeline_parallel/schedules.py", line 439, in forward_backward_no_pipelining
[rank6]:     output_tensor, num_tokens = forward_step(
[rank6]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/core/pipeline_parallel/schedules.py", line 264, in forward_step
[rank6]:     output_tensor, loss_func = forward_step_func(data_iterator, model)
[rank6]: TypeError: MegatronWrapTrainingFlowBase.forward_func() takes 2 positional arguments but 3 were given
[rank5]: Traceback (most recent call last):
[rank5]:   File "/gpfs/public/infra/qhu/projects/megatron-wrap/main.py", line 9, in <module>
[rank5]:     c.train([[]]*128)
[rank5]:   File "/gpfs/public/infra/qhu/projects/megatron-wrap/megatron_wrap/core/wrap.py", line 303, in train
[rank5]:     train_step(self.megatron_wrap_training_flow.forward_func,
[rank5]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/training/training.py", line 590, in train_step
[rank5]:     losses_reduced = forward_backward_func(
[rank5]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/core/pipeline_parallel/schedules.py", line 439, in forward_backward_no_pipelining
[rank5]:     output_tensor, num_tokens = forward_step(
[rank5]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/core/pipeline_parallel/schedules.py", line 264, in forward_step
[rank5]:     output_tensor, loss_func = forward_step_func(data_iterator, model)
[rank5]: TypeError: MegatronWrapTrainingFlowBase.forward_func() takes 2 positional arguments but 3 were given
[32m2024-12-21 10:36:51[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:219   [0m [34m[1m(rank0/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 92.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 11.8750
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 43.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.0000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 14.3125
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 62.2500
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.0000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.5000
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.8750
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.8750
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.6250
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.0000
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.0000
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.2500
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.1250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.7500
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.5000
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.0000
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.7500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.5000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.6250
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 32.5000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.2500
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.7500
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.0000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.2500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.0000
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 72.5000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.7500
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 37.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.0000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 39.7500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 43.2500
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.5000
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 94.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.7500
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.0000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.5000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 98.0000
[0m
[32m2024-12-21 10:36:51[0m [34m[1mDEBUG   [0m[34mwrap.py:train:300                       [0m [34m[1m(rank1/8) [WRAP] split dp, DP0/2 got 64 of 128[0m
[32m2024-12-21 10:36:51[0m [1mINFO    [0m[34mwrap.py:setup_model_and_optimizer:223   [0m [1m[STATUS] optimizer is sucessfully built: DistributedOptimizer(type=adam, lr=2e-05, min_lr=0.0, weight_decay=0.0adam_beta=(0.9,0.95), adam_eps=1e-08, sgd_momentum=0.9[0m
[32m2024-12-21 10:36:51[0m [1mINFO    [0m[34mwrap.py:setup_model_and_optimizer:224   [0m [1m[STATUS] scheduler is sucessfully built: OptimizerParamScheduler(lr_decay_style=cosine, lr_warmup_steps=409.6, lr_decay_steps=8192)[0m
[32m2024-12-21 10:36:51[0m [1mINFO    [0m[34mmain.py:<module>:8                      [0m [1m(rank0/8) namespace(tp_size=4, pp_size=1, dp_size=2, cp_size=1, ep_size=1, tp_rank=0, pp_rank=0, dp_rank=0, cp_rank=0, ep_rank=0)[0m
[32m2024-12-21 10:36:51[0m [34m[1mDEBUG   [0m[34mtimers.py:log:369                       [0m [34m[1m(rank7/8) (min, max) time across ranks (ms):
    load-checkpoint ................................: (17023.97, 17024.44)[0m
[32m2024-12-21 10:36:51[0m [1mINFO    [0m[34mwrap.py:_set_megatron_wrap_training_flow:282[0m [1m[STATUS] successfully set megatron wrap training flow <megatron_wrap.core.flow.MegatronWrapMockTrainingFlow object at 0x7fd47471b940>[0m
[rank4]: Traceback (most recent call last):
[rank4]:   File "/gpfs/public/infra/qhu/projects/megatron-wrap/main.py", line 9, in <module>
[rank4]:     c.train([[]]*128)
[rank4]:   File "/gpfs/public/infra/qhu/projects/megatron-wrap/megatron_wrap/core/wrap.py", line 303, in train
[rank4]:     train_step(self.megatron_wrap_training_flow.forward_func,
[rank4]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/training/training.py", line 590, in train_step
[rank4]:     losses_reduced = forward_backward_func(
[rank4]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/core/pipeline_parallel/schedules.py", line 439, in forward_backward_no_pipelining
[rank4]:     output_tensor, num_tokens = forward_step(
[rank4]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/core/pipeline_parallel/schedules.py", line 264, in forward_step
[rank4]:     output_tensor, loss_func = forward_step_func(data_iterator, model)
[rank4]: TypeError: MegatronWrapTrainingFlowBase.forward_func() takes 2 positional arguments but 3 were given
[32m2024-12-21 10:36:51[0m [34m[1mDEBUG   [0m[34mwrap.py:train:300                       [0m [34m[1m(rank0/8) [WRAP] split dp, DP0/2 got 64 of 128[0m
[rank1]: Traceback (most recent call last):
[rank1]:   File "/gpfs/public/infra/qhu/projects/megatron-wrap/main.py", line 9, in <module>
[rank1]:     c.train([[]]*128)
[rank1]:   File "/gpfs/public/infra/qhu/projects/megatron-wrap/megatron_wrap/core/wrap.py", line 303, in train
[rank1]:     train_step(self.megatron_wrap_training_flow.forward_func,
[rank1]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/training/training.py", line 590, in train_step
[rank1]:     losses_reduced = forward_backward_func(
[rank1]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/core/pipeline_parallel/schedules.py", line 439, in forward_backward_no_pipelining
[rank1]:     output_tensor, num_tokens = forward_step(
[rank1]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/core/pipeline_parallel/schedules.py", line 264, in forward_step
[rank1]:     output_tensor, loss_func = forward_step_func(data_iterator, model)
[rank1]: TypeError: MegatronWrapTrainingFlowBase.forward_func() takes 2 positional arguments but 3 were given
[rank0]: Traceback (most recent call last):
[rank0]:   File "/gpfs/public/infra/qhu/projects/megatron-wrap/main.py", line 9, in <module>
[rank0]:     c.train([[]]*128)
[rank0]:   File "/gpfs/public/infra/qhu/projects/megatron-wrap/megatron_wrap/core/wrap.py", line 303, in train
[rank0]:     train_step(self.megatron_wrap_training_flow.forward_func,
[rank0]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/training/training.py", line 590, in train_step
[rank0]:     losses_reduced = forward_backward_func(
[rank0]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/core/pipeline_parallel/schedules.py", line 439, in forward_backward_no_pipelining
[rank0]:     output_tensor, num_tokens = forward_step(
[rank0]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/core/pipeline_parallel/schedules.py", line 264, in forward_step
[rank0]:     output_tensor, loss_func = forward_step_func(data_iterator, model)
[rank0]: TypeError: MegatronWrapTrainingFlowBase.forward_func() takes 2 positional arguments but 3 were given
[32m2024-12-21 10:36:51[0m [34m[1mDEBUG   [0m[34mwrap.py:setup_model_and_optimizer:219   [0m [34m[1m(rank7/8) 
module.module.embedding.word_embeddings.weight: shape = torch.Size([8000, 4096]), norm = 99.0000
module.module.decoder.layers.0.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 18.3750
module.module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 2.4531
module.module.decoder.layers.0.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 52.2500
module.module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 3.5625
module.module.decoder.layers.0.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 77.5000
module.module.decoder.layers.0.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 56.5000
module.module.decoder.layers.1.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 19.0000
module.module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 5.9375
module.module.decoder.layers.1.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 92.5000
module.module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 6.5625
module.module.decoder.layers.1.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 84.0000
module.module.decoder.layers.1.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.2.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.7500
module.module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 11.3750
module.module.decoder.layers.2.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 8.6875
module.module.decoder.layers.2.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 86.0000
module.module.decoder.layers.2.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.3.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 24.8750
module.module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 18.3750
module.module.decoder.layers.3.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.5000
module.module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.0000
module.module.decoder.layers.3.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.0000
module.module.decoder.layers.3.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.4.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.3750
module.module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 16.8750
module.module.decoder.layers.4.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 86.5000
module.module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 11.9375
module.module.decoder.layers.4.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 87.5000
module.module.decoder.layers.4.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.5.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 30.3750
module.module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 17.2500
module.module.decoder.layers.5.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 86.5000
module.module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 12.6875
module.module.decoder.layers.5.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.5.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.5000
module.module.decoder.layers.6.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.8750
module.module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.3750
module.module.decoder.layers.6.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 13.5625
module.module.decoder.layers.6.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.6.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.7.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 28.7500
module.module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.7.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.5000
module.module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.3125
module.module.decoder.layers.7.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.7.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 58.2500
module.module.decoder.layers.8.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 26.7500
module.module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 21.7500
module.module.decoder.layers.8.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.0000
module.module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.6250
module.module.decoder.layers.8.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.8.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.0000
module.module.decoder.layers.9.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.6250
module.module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.1250
module.module.decoder.layers.9.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 80.5000
module.module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 14.9375
module.module.decoder.layers.9.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.0000
module.module.decoder.layers.9.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.2500
module.module.decoder.layers.10.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.6250
module.module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 22.3750
module.module.decoder.layers.10.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 81.0000
module.module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.1875
module.module.decoder.layers.10.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.10.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 59.7500
module.module.decoder.layers.11.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 29.8750
module.module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.6250
module.module.decoder.layers.11.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 15.6875
module.module.decoder.layers.11.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 88.5000
module.module.decoder.layers.11.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.2500
module.module.decoder.layers.12.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 27.6250
module.module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.12.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.1250
module.module.decoder.layers.12.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.12.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 60.7500
module.module.decoder.layers.13.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.13.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 16.6250
module.module.decoder.layers.13.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.13.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.14.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.2500
module.module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.7500
module.module.decoder.layers.14.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.0000
module.module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 17.3750
module.module.decoder.layers.14.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.0000
module.module.decoder.layers.14.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.15.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 31.5000
module.module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 25.1250
module.module.decoder.layers.15.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 79.0000
module.module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 18.0000
module.module.decoder.layers.15.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 89.5000
module.module.decoder.layers.15.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.16.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.5000
module.module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.0000
module.module.decoder.layers.16.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 19.1250
module.module.decoder.layers.16.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.16.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.17.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 33.0000
module.module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 26.7500
module.module.decoder.layers.17.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 74.5000
module.module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 20.3750
module.module.decoder.layers.17.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.0000
module.module.decoder.layers.17.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.18.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.0000
module.module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.2500
module.module.decoder.layers.18.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 21.5000
module.module.decoder.layers.18.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.18.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.2500
module.module.decoder.layers.19.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.0000
module.module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.1250
module.module.decoder.layers.19.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.5000
module.module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 22.2500
module.module.decoder.layers.19.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.19.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.20.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 35.7500
module.module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 28.3750
module.module.decoder.layers.20.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.0000
module.module.decoder.layers.20.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 90.5000
module.module.decoder.layers.20.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.21.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 34.7500
module.module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.1250
module.module.decoder.layers.21.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 70.0000
module.module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 23.8750
module.module.decoder.layers.21.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.0000
module.module.decoder.layers.21.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.22.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.5000
module.module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.22.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.0000
module.module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 24.7500
module.module.decoder.layers.22.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.22.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.5000
module.module.decoder.layers.23.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.5000
module.module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.7500
module.module.decoder.layers.23.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 25.5000
module.module.decoder.layers.23.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.23.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 61.7500
module.module.decoder.layers.24.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 36.7500
module.module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 32.2500
module.module.decoder.layers.24.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 26.3750
module.module.decoder.layers.24.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.24.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.0000
module.module.decoder.layers.25.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.5000
module.module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.25.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 77.5000
module.module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.1250
module.module.decoder.layers.25.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 91.5000
module.module.decoder.layers.25.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.2500
module.module.decoder.layers.26.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 38.2500
module.module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.0000
module.module.decoder.layers.26.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 70.5000
module.module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.0000
module.module.decoder.layers.26.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.26.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.5000
module.module.decoder.layers.27.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.2500
module.module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.0000
module.module.decoder.layers.27.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 28.8750
module.module.decoder.layers.27.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 92.5000
module.module.decoder.layers.27.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 62.7500
module.module.decoder.layers.28.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.28.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 35.5000
module.module.decoder.layers.28.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 78.0000
module.module.decoder.layers.28.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 29.5000
module.module.decoder.layers.28.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.0000
module.module.decoder.layers.28.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.2500
module.module.decoder.layers.29.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.5000
module.module.decoder.layers.29.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 34.5000
module.module.decoder.layers.29.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 73.0000
module.module.decoder.layers.29.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.3750
module.module.decoder.layers.29.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 93.5000
module.module.decoder.layers.29.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.5000
module.module.decoder.layers.30.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 42.5000
module.module.decoder.layers.30.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 36.0000
module.module.decoder.layers.30.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 76.0000
module.module.decoder.layers.30.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 30.6250
module.module.decoder.layers.30.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 94.0000
module.module.decoder.layers.30.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.layers.31.self_attention.linear_proj.weight: shape = torch.Size([4096, 1024]), norm = 40.5000
module.module.decoder.layers.31.self_attention.linear_qkv.layer_norm_weight: shape = torch.Size([4096]), norm = 29.6250
module.module.decoder.layers.31.self_attention.linear_qkv.weight: shape = torch.Size([3072, 4096]), norm = 75.5000
module.module.decoder.layers.31.mlp.linear_fc1.layer_norm_weight: shape = torch.Size([4096]), norm = 27.3750
module.module.decoder.layers.31.mlp.linear_fc1.weight: shape = torch.Size([5504, 4096]), norm = 99.0000
module.module.decoder.layers.31.mlp.linear_fc2.weight: shape = torch.Size([4096, 2752]), norm = 63.0000
module.module.decoder.final_layernorm.weight: shape = torch.Size([4096]), norm = 114.5000
module.module.output_layer.weight: shape = torch.Size([8000, 4096]), norm = 100.0000
[0m
[32m2024-12-21 10:36:51[0m [1mINFO    [0m[34mmain.py:<module>:8                      [0m [1m(rank7/8) namespace(tp_size=4, pp_size=1, dp_size=2, cp_size=1, ep_size=1, tp_rank=3, pp_rank=0, dp_rank=1, cp_rank=0, ep_rank=0)[0m
[32m2024-12-21 10:36:51[0m [34m[1mDEBUG   [0m[34mwrap.py:train:300                       [0m [34m[1m(rank7/8) [WRAP] split dp, DP1/2 got 64 of 128[0m
[rank7]: Traceback (most recent call last):
[rank7]:   File "/gpfs/public/infra/qhu/projects/megatron-wrap/main.py", line 9, in <module>
[rank7]:     c.train([[]]*128)
[rank7]:   File "/gpfs/public/infra/qhu/projects/megatron-wrap/megatron_wrap/core/wrap.py", line 303, in train
[rank7]:     train_step(self.megatron_wrap_training_flow.forward_func,
[rank7]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/training/training.py", line 590, in train_step
[rank7]:     losses_reduced = forward_backward_func(
[rank7]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/core/pipeline_parallel/schedules.py", line 439, in forward_backward_no_pipelining
[rank7]:     output_tensor, num_tokens = forward_step(
[rank7]:   File "/gpfs/public/infra/qhu/projects/megatron_core_080/megatron/core/pipeline_parallel/schedules.py", line 264, in forward_step
[rank7]:     output_tensor, loss_func = forward_step_func(data_iterator, model)
[rank7]: TypeError: MegatronWrapTrainingFlowBase.forward_func() takes 2 positional arguments but 3 were given
[rank7]:[W1221 22:36:51.224666588 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W1221 22:36:51.382274608 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank3]:[W1221 22:36:51.385018945 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W1221 22:36:51.392933534 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank1]:[W1221 22:36:51.393781457 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank6]:[W1221 22:36:51.407862764 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank4]:[W1221 22:36:51.408361996 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank5]:[W1221 22:36:51.412858257 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W1221 22:36:52.950000 139636510705472 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 177430 closing signal SIGTERM
W1221 22:36:52.955000 139636510705472 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 177431 closing signal SIGTERM
W1221 22:36:52.958000 139636510705472 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 177432 closing signal SIGTERM
W1221 22:36:52.964000 139636510705472 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 177433 closing signal SIGTERM
W1221 22:36:52.995000 139636510705472 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 177434 closing signal SIGTERM
W1221 22:36:52.998000 139636510705472 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 177435 closing signal SIGTERM
W1221 22:36:53.037000 139636510705472 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 177436 closing signal SIGTERM
E1221 22:36:54.102000 139636510705472 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 7 (pid: 177437) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.4.0+cu124', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-21_22:36:52
  host      : 171dbf39c5b8
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 177437)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
