megatron_lm:
  model:
    arch:
      __select__: aria-moe
      __override__:
        # num_layers: 8
        hidden_size: 2048
        ffn_hidden_size: 10240
        num_attention_heads: 8
    parallel:
      __select__: base
  train:
    dtype:
      __select__: bf16
    common:
      micro_batch_size: 100
      global_batch_size: 16
      seq_length: 1024
      train_iters: 32
  misc:
    data:
      mock_data: true
megatron_wrap:
  output:
    __select__: value

    