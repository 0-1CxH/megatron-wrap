megatron_lm:
  model:
    arch:
      __select__: llama2-7b
    parallel:
      __select__: base
      __override__:
        tensor_model_parallel_size: 4
        pipeline_model_parallel_size: 1
  train:
    common:
      micro_batch_size: 4
      global_batch_size: 128
      seq_length: 512
      train_iters: 64
      load: ckpt/llama-2-7b-mcore-tp4pp1
    learning-rate:
      lr: 2.0e-5
      lr_warmup_fraction: 0.05
megatron_wrap:
  output:
    __select__: value

    