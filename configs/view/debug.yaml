megatron-lm:
  model:
    arch:
      __select__: base
      __override__:
        num_layers: 8
        hidden_size: 2048
        ffn_hidden_size: 10240
        num_attention_heads: 8
    parallel:
      __select__: base
  train:
    dtype:
      __select__: bf16
    common:
      micro_batch_size: 100
      global_batch_size: 16
      seq_length: 1024
      train_iters: 32
megatron-wrap:
  output:
    __select__: value

    